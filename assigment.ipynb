{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Estimate the emission parameters from the training set using MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def emission_params(tokens_list: list, y: str, x: str, special_token='#UNK#'):\n",
    "    y_count = 0\n",
    "    x_given_y_count = 0\n",
    "    x_count = 0\n",
    "    for token in tokens_list: \n",
    "        if token[1] == y:\n",
    "            y_count += 1\n",
    "            if token[0] == x:\n",
    "                x_given_y_count += 1\n",
    "        if token[0] == x:\n",
    "            x_count += 1\n",
    "    \n",
    "    if x_count == 0:\n",
    "        return emission_params(tokens_list, y, special_token)\n",
    "    return float(x_given_y_count) / y_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example ussage\n",
    "import os\n",
    "\n",
    "data = [['a', 'O'], ['b', 'O'], ['a', 'I'], ['c', 'O']]\n",
    "emission_params(tokens_list=data, y='O', x='a')\n",
    "\n",
    "# with open('./EN/train') as train_file:\n",
    "#     read_data = train_file.read()\n",
    "#     read_data = os.linesep.join([s for s in read_data.splitlines() if s])\n",
    "#     data = list(map(lambda x: x.split(' '),read_data.split('\\n')))\n",
    "# emission_params(tokens_list=data, y='O', x='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify training set to replace words that appear less than k times with special token. Apply this to the emission parameters prediction function with k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(tokens_list, k = 1, special_token = '#UNK#'):\n",
    "    token_freq = {}\n",
    "    for token in tokens_list:\n",
    "        if token[0] not in token_freq: \n",
    "            token_freq[token[0]] = 1\n",
    "        else:\n",
    "            token_freq[token[0]] += 1\n",
    "    for i in range(len(tokens_list)):\n",
    "        if token_freq[tokens_list[i][0]] < k:\n",
    "            tokens_list[i][0] = special_token\n",
    "    \n",
    "    return tokens_list\n",
    "\n",
    "def emission_params_clean_data(tokens_list: list, y: str, x: str):\n",
    "    tokens_list = clean_data(tokens_list = tokens_list, k = 3)\n",
    "    return emission_params(tokens_list, y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "data = [['a', 'O'], ['b', 'O'], ['a', 'I'], ['c', 'O'], ['a', 'O']]\n",
    "emission_params_clean_data(tokens_list=data, y='O', x='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis system that produces the tag for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: EN\n",
      "Finished: SG\n",
      "Finished: CN\n",
      "Finished: FR\n"
     ]
    }
   ],
   "source": [
    "def predict_tag(tokens_list: list, x: str):\n",
    "    y_tags = ['O', 'B-positive', 'I-positive', 'B-neutral', 'I-neutral', 'B-negative', 'I-negative']\n",
    "    score = 0.0\n",
    "    y_tag = None\n",
    "    for y in y_tags:\n",
    "        y_score = emission_params_clean_data(tokens_list, y, x)\n",
    "        if (y_score > score):\n",
    "            y_tag = y\n",
    "            score = y_score \n",
    "    return y_tag\n",
    "\n",
    "# clean data before predicting the tag\n",
    "def predict_tag_optimized(tokens_list: list, x: str):\n",
    "    y_tags = ['O', 'B-positive', 'I-positive', 'B-neutral', 'I-neutral', 'B-negative', 'I-negative']\n",
    "    score = 0.0\n",
    "    y_tag = None\n",
    "    for y in y_tags:\n",
    "        y_score = emission_params(tokens_list, y, x)\n",
    "        if (y_score > score):\n",
    "            y_tag = y\n",
    "            score = y_score \n",
    "    return y_tag\n",
    "languages = ['EN', 'SG', 'CN', 'FR']\n",
    "\n",
    "for l in languages:\n",
    "    with open(\"./{}/train\".format(l)) as train_file:\n",
    "        read_data = train_file.read()\n",
    "        read_data = os.linesep.join([s for s in read_data.splitlines() if s])\n",
    "        data = list(map(lambda x: x.split(' '),read_data.split('\\n')))\n",
    "        data_cleaned = clean_data(tokens_list = data, k = 3)\n",
    "\n",
    "    with open(\"./{}/dev.in\".format(l)) as in_file, open(\"./{}/dev.p2.out\".format(l), 'w+') as out_file:\n",
    "        for line in in_file:\n",
    "            word = line.strip()\n",
    "            if (word == ''):\n",
    "                out_file.write(\"\\n\")\n",
    "            else:\n",
    "                out_file.write(\"{} {}\\n\".format(word, predict_tag_optimized(data_cleaned, word)))\n",
    "    print(\"Finished: {}\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: EN\n",
      "\n",
      "#Entity in gold data: 226\n",
      "#Entity in prediction: 1201\n",
      "\n",
      "#Correct Entity : 165\n",
      "Entity  precision: 0.1374\n",
      "Entity  recall: 0.7301\n",
      "Entity  F: 0.2313\n",
      "\n",
      "#Correct Sentiment : 71\n",
      "Sentiment  precision: 0.0591\n",
      "Sentiment  recall: 0.3142\n",
      "Sentiment  F: 0.0995\n",
      "\n",
      "----------------------\n",
      "Language: SG\n",
      "\n",
      "#Entity in gold data: 1382\n",
      "#Entity in prediction: 6599\n",
      "\n",
      "#Correct Entity : 794\n",
      "Entity  precision: 0.1203\n",
      "Entity  recall: 0.5745\n",
      "Entity  F: 0.1990\n",
      "\n",
      "#Correct Sentiment : 315\n",
      "Sentiment  precision: 0.0477\n",
      "Sentiment  recall: 0.2279\n",
      "Sentiment  F: 0.0789\n",
      "\n",
      "----------------------\n",
      "Language: CN\n",
      "\n",
      "#Entity in gold data: 362\n",
      "#Entity in prediction: 3318\n",
      "\n",
      "#Correct Entity : 183\n",
      "Entity  precision: 0.0552\n",
      "Entity  recall: 0.5055\n",
      "Entity  F: 0.0995\n",
      "\n",
      "#Correct Sentiment : 57\n",
      "Sentiment  precision: 0.0172\n",
      "Sentiment  recall: 0.1575\n",
      "Sentiment  F: 0.0310\n",
      "\n",
      "----------------------\n",
      "Language: FR\n",
      "\n",
      "#Entity in gold data: 223\n",
      "#Entity in prediction: 1149\n",
      "\n",
      "#Correct Entity : 182\n",
      "Entity  precision: 0.1584\n",
      "Entity  recall: 0.8161\n",
      "Entity  F: 0.2653\n",
      "\n",
      "#Correct Sentiment : 68\n",
      "Sentiment  precision: 0.0592\n",
      "Sentiment  recall: 0.3049\n",
      "Sentiment  F: 0.0991\n",
      "\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "for l in languages:\n",
    "    output = os.popen(\"python EvalScript/evalResult.py {0}/dev.out {0}/dev.p2.out\".format(l)).read()\n",
    "    print(\"Language: {}\".format(l))\n",
    "    print(output)\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimates the transition parameters from the training set using MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "def transition_params(ordered_words_list: list):\n",
    "    count = {}\n",
    "    count_given = {} # 2 layer dictionary depth-0 key is the (i-1)-word, depth-1 key is the i-word\n",
    "    \n",
    "    # count frequency of all word and combinations of 2 words in the dataset\n",
    "    for idx, word in enumerate(ordered_words_list):\n",
    "        if word not in count:\n",
    "            count[word] = 1\n",
    "            count_given[word] = {}\n",
    "            if idx < len(ordered_words_list) - 1:\n",
    "                next_word = ordered_words_list[idx + 1]\n",
    "                count_given[word][next_word] = 1\n",
    "        else:\n",
    "            count[word] += 1\n",
    "            if idx < len(ordered_words_list) - 1:\n",
    "                next_word = ordered_words_list[idx + 1]\n",
    "                if next_word not in count_given[word]:\n",
    "                    count_given[word][next_word] = 1\n",
    "                else:\n",
    "                    count_given[word][next_word] += 1\n",
    "    \n",
    "    # calculate trans_params\n",
    "    trans_params = copy.deepcopy(count_given)\n",
    "    for given_word in trans_params:\n",
    "        for word in trans_params[given_word]:\n",
    "            trans_params[given_word][word] /= count[given_word]\n",
    "            \n",
    "    return trans_params\n",
    "\n",
    "def specific_transition_params(ordered_words_list: list, y: str, y_given: str):\n",
    "    trans_params = transition_params(ordered_words_list)\n",
    "    if y not in trans_params:\n",
    "        return 0;\n",
    "    elif y_given not in trans_params[y]:\n",
    "        return 0;\n",
    "    else:\n",
    "        return trans_params[y_given][y]\n",
    "    \n",
    "specific_transition_params(['a', 'b', 'b', 'c', 'b', 'a', 'd', 'h', 'b'], 'b', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viterbi algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Again': 'I',\n",
       " 'And': 'I,',\n",
       " 'But': 'I',\n",
       " 'Drink': 'too',\n",
       " 'Hey': 'I',\n",
       " 'Hey,': 'you',\n",
       " 'I': \"can't\",\n",
       " \"I'm\": 'okay',\n",
       " 'I,': 'I',\n",
       " 'Moved': 'to',\n",
       " 'No,': 'I,',\n",
       " 'Now': \"you're\",\n",
       " 'START': 'Hey',\n",
       " 'STOP': '',\n",
       " 'a': 'hotel',\n",
       " 'an': 'issue',\n",
       " 'and': \"that's\",\n",
       " 'bar': 'And',\n",
       " 'before': 'I',\n",
       " 'breaks': 'your',\n",
       " 'broke-down': 'car',\n",
       " 'calls': 'Now',\n",
       " \"can't\": 'stop',\n",
       " 'car': 'And',\n",
       " 'city': 'in',\n",
       " 'doing': 'just',\n",
       " 'fine': 'before',\n",
       " 'four': 'years,',\n",
       " 'friends': 'it',\n",
       " 'heart': 'Moved',\n",
       " 'hope': 'I',\n",
       " 'hotel': 'bar',\n",
       " 'in': 'a',\n",
       " 'issue': 'But',\n",
       " 'it': 'breaks',\n",
       " 'just': 'fine',\n",
       " 'know': 'it',\n",
       " 'looking': 'pretty',\n",
       " 'meet': 'them',\n",
       " 'met': 'you',\n",
       " 'much': 'and',\n",
       " 'never': 'see',\n",
       " 'nice': 'to',\n",
       " 'no': 'calls',\n",
       " 'okay': 'Hey,',\n",
       " 'pretty': 'in',\n",
       " 'see': 'them',\n",
       " 'stop': 'STOP',\n",
       " 'tell': 'your',\n",
       " \"that's\": 'an',\n",
       " 'the': 'city',\n",
       " 'them': 'Again',\n",
       " 'to': 'the',\n",
       " 'too': 'much',\n",
       " 'was': 'nice',\n",
       " 'years,': 'no',\n",
       " 'you': 'tell',\n",
       " \"you're\": 'looking',\n",
       " 'your': 'heart'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def viterbi(sentence: str):\n",
    "    words_list = ['START'] + sentence.split() + ['STOP']\n",
    "    trans_params = transition_params(words_list)\n",
    "    result = {} # key is given_word, value is the the maximum-likely next word\n",
    "    \n",
    "    for given_word in trans_params:\n",
    "        max_arg = 0\n",
    "        result[given_word] = ''\n",
    "        for word in trans_params[given_word]:\n",
    "            if trans_params[given_word][word] > max_arg:\n",
    "                result[given_word] = word\n",
    "        \n",
    "    return result\n",
    "\n",
    "viterbi(\"\"\"\n",
    "Hey I was doing just fine before I met you\n",
    "Drink too much and that\\'s an issue\n",
    "But I\\'m okay\n",
    "Hey, you tell your friends it was nice to meet them\n",
    "But I hope I never see them\n",
    "Again\n",
    "I know it breaks your heart\n",
    "Moved to the city in a broke-down car\n",
    "And four years, no calls\n",
    "Now you're looking pretty in a hotel bar\n",
    "And I, I, I, I, I can't stop\n",
    "No, I, I, I, I, I can't stop\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
