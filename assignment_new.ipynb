{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hmm import SUTDHMM\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Estimate the emission parameters from the training set using MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish training for EN\n",
      "Finished: EN\n",
      "Language: EN\n",
      "\n",
      "#Entity in gold data: 226\n",
      "#Entity in prediction: 706\n",
      "\n",
      "#Correct Entity : 133\n",
      "Entity  precision: 0.1884\n",
      "Entity  recall: 0.5885\n",
      "Entity  F: 0.2854\n",
      "\n",
      "#Correct Sentiment : 45\n",
      "Sentiment  precision: 0.0637\n",
      "Sentiment  recall: 0.1991\n",
      "Sentiment  F: 0.0966\n",
      "\n",
      "----------------------\n",
      "Finish training for SG\n",
      "Finished: SG\n",
      "Language: SG\n",
      "\n",
      "#Entity in gold data: 1382\n",
      "#Entity in prediction: 2764\n",
      "\n",
      "#Correct Entity : 511\n",
      "Entity  precision: 0.1849\n",
      "Entity  recall: 0.3698\n",
      "Entity  F: 0.2465\n",
      "\n",
      "#Correct Sentiment : 272\n",
      "Sentiment  precision: 0.0984\n",
      "Sentiment  recall: 0.1968\n",
      "Sentiment  F: 0.1312\n",
      "\n",
      "----------------------\n",
      "Finish training for CN\n",
      "Finished: CN\n",
      "Language: CN\n",
      "\n",
      "#Entity in gold data: 362\n",
      "#Entity in prediction: 1688\n",
      "\n",
      "#Correct Entity : 114\n",
      "Entity  precision: 0.0675\n",
      "Entity  recall: 0.3149\n",
      "Entity  F: 0.1112\n",
      "\n",
      "#Correct Sentiment : 71\n",
      "Sentiment  precision: 0.0421\n",
      "Sentiment  recall: 0.1961\n",
      "Sentiment  F: 0.0693\n",
      "\n",
      "----------------------\n",
      "Finish training for FR\n",
      "Finished: FR\n",
      "Language: FR\n",
      "\n",
      "#Entity in gold data: 223\n",
      "#Entity in prediction: 691\n",
      "\n",
      "#Correct Entity : 174\n",
      "Entity  precision: 0.2518\n",
      "Entity  recall: 0.7803\n",
      "Entity  F: 0.3807\n",
      "\n",
      "#Correct Sentiment : 65\n",
      "Sentiment  precision: 0.0941\n",
      "Sentiment  recall: 0.2915\n",
      "Sentiment  F: 0.1422\n",
      "\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "from hmm import SUTDHMM\n",
    "languages = ['EN', 'SG', 'CN', 'FR']\n",
    "\n",
    "for l in languages:\n",
    "    model = SUTDHMM()\n",
    "    model.train(input_filename='./{}/train'.format(l))\n",
    "    print(\"Finish training for {}\".format(l))\n",
    "\n",
    "    with open(\"./{}/dev.in\".format(l)) as in_file, open(\"./{}/dev.p2.out\".format(l), 'w+') as out_file:\n",
    "        for line in in_file:\n",
    "            word = line.strip()\n",
    "            if (word == ''):\n",
    "                out_file.write(\"\\n\")\n",
    "            else:\n",
    "                out_file.write(\"{} {}\\n\".format(word, model.predict_label_using_emission(word)))\n",
    "    print(\"Finished: {}\".format(l))\n",
    "    \n",
    "    output = os.popen(\"python3 EvalScript/evalResult.py {0}/dev.out {0}/dev.p2.out\".format(l)).read()\n",
    "    print(\"Language: {}\".format(l))\n",
    "    print(output)\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish training for EN\n",
      "----------Viterbi for EN------------\n",
      "Viterbi Finished: EN\n",
      "Language: EN\n",
      "\n",
      "#Entity in gold data: 226\n",
      "#Entity in prediction: 116\n",
      "\n",
      "#Correct Entity : 74\n",
      "Entity  precision: 0.6379\n",
      "Entity  recall: 0.3274\n",
      "Entity  F: 0.4327\n",
      "\n",
      "#Correct Sentiment : 51\n",
      "Sentiment  precision: 0.4397\n",
      "Sentiment  recall: 0.2257\n",
      "Sentiment  F: 0.2982\n",
      "\n",
      "Finish training for SG\n",
      "----------Viterbi for SG------------\n",
      "Viterbi Finished: SG\n",
      "Language: SG\n",
      "\n",
      "#Entity in gold data: 1382\n",
      "#Entity in prediction: 499\n",
      "\n",
      "#Correct Entity : 228\n",
      "Entity  precision: 0.4569\n",
      "Entity  recall: 0.1650\n",
      "Entity  F: 0.2424\n",
      "\n",
      "#Correct Sentiment : 152\n",
      "Sentiment  precision: 0.3046\n",
      "Sentiment  recall: 0.1100\n",
      "Sentiment  F: 0.1616\n",
      "\n",
      "Finish training for CN\n",
      "----------Viterbi for CN------------\n",
      "Viterbi Finished: CN\n",
      "Language: CN\n",
      "\n",
      "#Entity in gold data: 362\n",
      "#Entity in prediction: 173\n",
      "\n",
      "#Correct Entity : 28\n",
      "Entity  precision: 0.1618\n",
      "Entity  recall: 0.0773\n",
      "Entity  F: 0.1047\n",
      "\n",
      "#Correct Sentiment : 17\n",
      "Sentiment  precision: 0.0983\n",
      "Sentiment  recall: 0.0470\n",
      "Sentiment  F: 0.0636\n",
      "\n",
      "Finish training for FR\n",
      "----------Viterbi for FR------------\n",
      "Viterbi Finished: FR\n",
      "Language: FR\n",
      "\n",
      "#Entity in gold data: 223\n",
      "#Entity in prediction: 115\n",
      "\n",
      "#Correct Entity : 79\n",
      "Entity  precision: 0.6870\n",
      "Entity  recall: 0.3543\n",
      "Entity  F: 0.4675\n",
      "\n",
      "#Correct Sentiment : 51\n",
      "Sentiment  precision: 0.4435\n",
      "Sentiment  recall: 0.2287\n",
      "Sentiment  F: 0.3018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "languages = ['EN', 'SG', 'CN', 'FR']\n",
    "\n",
    "for l in languages:\n",
    "    model = SUTDHMM()\n",
    "    model.train(input_filename='./{}/train'.format(l))\n",
    "\n",
    "    print(\"Finish training for {}\".format(l))\n",
    "\n",
    "    print(\"----------Viterbi for {0}------------\".format(l))\n",
    "    with open(\"./{}/dev.in\".format(l)) as in_file, open(\"./{}/dev.p3.out\".format(l), 'w+') as out_file:\n",
    "        read_data = in_file.read()\n",
    "        sentences = list(filter(lambda x: len(x) > 0, read_data.split('\\n\\n')))\n",
    "        sentences = list(map(lambda x: ' '.join(x.split('\\n')), sentences))\n",
    "        for sentence in sentences:\n",
    "            sentence_labels, chance = model.viterbi(sentence)\n",
    "            for idx, word in enumerate(sentence.split()):\n",
    "                out_file.write(\"{} {}\\n\".format(word, sentence_labels[idx]))\n",
    "            out_file.write('\\n')\n",
    "        out_file.close()\n",
    "        in_file.close()\n",
    "\n",
    "    print(\"Viterbi Finished: {}\".format(l))\n",
    "    \n",
    "    output = os.popen(\n",
    "        \"python3 EvalScript/evalResult.py {0}/dev.out {0}/dev.p3.out\".format(l)).read()\n",
    "    print(\"Language: {}\".format(l))\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish training for EN\n",
      "----------Viterbi for EN------------\n",
      "Improved Viterbi Finished: EN\n",
      "Language: EN\n",
      "\n",
      "#Entity in gold data: 226\n",
      "#Entity in prediction: 98\n",
      "\n",
      "#Correct Entity : 62\n",
      "Entity  precision: 0.6327\n",
      "Entity  recall: 0.2743\n",
      "Entity  F: 0.3827\n",
      "\n",
      "#Correct Sentiment : 41\n",
      "Sentiment  precision: 0.4184\n",
      "Sentiment  recall: 0.1814\n",
      "Sentiment  F: 0.2531\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-604e7dab4784>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlanguages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSUTDHMM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstandardise_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./{}/train'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finish training for {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/m-hmm/hmm.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, raw_string, input_filename)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_emission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/m-hmm/hmm.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, raw_string, data_filename)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "languages = ['EN', 'SG', 'CN', 'FR']\n",
    "\n",
    "for l in languages:\n",
    "    model = SUTDHMM(standardise_word=True)\n",
    "    model.train(input_filename='./{}/train'.format(l))\n",
    "\n",
    "    print(\"Finish training for {}\".format(l))\n",
    "\n",
    "    print(\"----------Viterbi for {0}------------\".format(l))\n",
    "    with open(\"./{}/dev.in\".format(l)) as in_file, open(\"./{}/dev.p3.out\".format(l), 'w+') as out_file:\n",
    "        read_data = in_file.read()\n",
    "        sentences = list(filter(lambda x: len(x) > 0, read_data.split('\\n\\n')))\n",
    "        sentences = list(map(lambda x: ' '.join(x.split('\\n')), sentences))\n",
    "        for sentence in sentences:\n",
    "            sentence_labels, chance = model.viterbi(sentence)\n",
    "            for idx, word in enumerate(sentence.split()):\n",
    "                out_file.write(\"{} {}\\n\".format(word, sentence_labels[idx]))\n",
    "            out_file.write('\\n')\n",
    "        out_file.close()\n",
    "        in_file.close()\n",
    "\n",
    "    print(\"Improved Viterbi Finished: {}\".format(l))\n",
    "    \n",
    "    output = os.popen(\n",
    "        \"python3 EvalScript/evalResult.py {0}/dev.out {0}/dev.p3.out\".format(l)).read()\n",
    "    print(\"Language: {}\".format(l))\n",
    "    print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
