{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Estimate the emission parameters from the training set using MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def emission_params_cache(tokens_list: list):\n",
    "    y_count = {}\n",
    "    x_given_y_count = {} \n",
    "    \n",
    "    for token in tokens_list:\n",
    "        if token[1] not in y_count:\n",
    "            y_count[token[1]] = 1\n",
    "            x_given_y_count[token[1]] = {}\n",
    "            x_given_y_count[token[1]][token[0]] = 1\n",
    "        else:\n",
    "            y_count[token[1]] += 1\n",
    "            if token[0] not in x_given_y_count[token[1]]:\n",
    "                x_given_y_count[token[1]][token[0]] = 1\n",
    "            else:\n",
    "                x_given_y_count[token[1]][token[0]] += 1\n",
    "    \n",
    "    # calculate emission params\n",
    "    emission_p = copy.deepcopy(x_given_y_count)\n",
    "    for label in emission_p:\n",
    "        for word in emission_p[label]:\n",
    "            emission_p[label][word] = float(x_given_y_count[label][word]) / y_count[label]\n",
    "    \n",
    "    return emission_p\n",
    "    \n",
    "def emission_params(tokens_list: list, y: str, x: str, special_token='#UNK#'):\n",
    "    emission_p = emission_params_cache(tokens_list)\n",
    "    x_inside = False\n",
    "    for token in tokens_list:\n",
    "        if token[0] == x:\n",
    "            x_inside = True\n",
    "            break\n",
    "    \n",
    "    if not x_inside:\n",
    "        x = special_token\n",
    "\n",
    "    if x not in emission_p[y]:\n",
    "        return 0\n",
    "    else:\n",
    "        return emission_p[y][x]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': {'a': 0.3333333333333333, 'b': 0.3333333333333333, 'c': 0.3333333333333333}, 'I': {'a': 1.0}}\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Example ussage\n",
    "import os\n",
    "\n",
    "data = [['a', 'O'], ['b', 'O'], ['a', 'I'], ['c', 'O']]\n",
    "print(emission_params_cache(data))\n",
    "print(emission_params(tokens_list=data, y='I', x='b'))\n",
    "\n",
    "# with open('./EN/train') as train_file:\n",
    "#     read_data = train_file.read()\n",
    "#     read_data = os.linesep.join([s for s in read_data.splitlines() if s])\n",
    "#     data = list(map(lambda x: x.split(' '),read_data.split('\\n')))\n",
    "# emission_params(tokens_list=data, y='O', x='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify training set to replace words that appear less than k times with special token. Apply this to the emission parameters prediction function with k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(tokens_list, k = 1, special_token = '#UNK#'):\n",
    "    token_freq = {}\n",
    "    for token in tokens_list:\n",
    "        if token[0] not in token_freq: \n",
    "            token_freq[token[0]] = 1\n",
    "        else:\n",
    "            token_freq[token[0]] += 1\n",
    "    for i in range(len(tokens_list)):\n",
    "        if token_freq[tokens_list[i][0]] < k:\n",
    "            tokens_list[i][0] = special_token\n",
    "    \n",
    "    return tokens_list\n",
    "\n",
    "def emission_params_clean_data(tokens_list: list, y: str, x: str):\n",
    "    tokens_list = clean_data(tokens_list = tokens_list, k = 3)\n",
    "    return emission_params(tokens_list, y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "data = [['a', 'O'], ['b', 'O'], ['a', 'I'], ['c', 'O'], ['a', 'O']]\n",
    "emission_params_clean_data(tokens_list=data, y='O', x='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis system that produces the tag for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: EN\n",
      "Finished: SG\n",
      "Finished: CN\n",
      "Finished: FR\n"
     ]
    }
   ],
   "source": [
    "def predict_tag(tokens_list: list, x: str):\n",
    "    y_tags = ['O', 'B-positive', 'I-positive', 'B-neutral', 'I-neutral', 'B-negative', 'I-negative']\n",
    "    score = 0.0\n",
    "    y_tag = None\n",
    "    for y in y_tags:\n",
    "        y_score = emission_params_clean_data(tokens_list, y, x)\n",
    "        if (y_score > score):\n",
    "            y_tag = y\n",
    "            score = y_score \n",
    "    return y_tag\n",
    "\n",
    "# clean data before predicting the tag\n",
    "def predict_tag_optimized(tokens_list: list, x: str):\n",
    "    y_tags = ['O', 'B-positive', 'I-positive', 'B-neutral', 'I-neutral', 'B-negative', 'I-negative']\n",
    "    score = 0.0\n",
    "    y_tag = None\n",
    "    for y in y_tags:\n",
    "        y_score = emission_params(tokens_list, y, x)\n",
    "        if (y_score > score):\n",
    "            y_tag = y\n",
    "            score = y_score \n",
    "    return y_tag\n",
    "languages = ['EN', 'SG', 'CN', 'FR']\n",
    "\n",
    "for l in languages:\n",
    "    with open(\"./{}/train\".format(l)) as train_file:\n",
    "        read_data = train_file.read()\n",
    "        read_data = os.linesep.join([s for s in read_data.splitlines() if s])\n",
    "        data = list(map(lambda x: x.split(' '),read_data.split('\\n')))\n",
    "        data_cleaned = clean_data(tokens_list = data, k = 3)\n",
    "\n",
    "    with open(\"./{}/dev.in\".format(l)) as in_file, open(\"./{}/dev.p2.out\".format(l), 'w+') as out_file:\n",
    "        for line in in_file:\n",
    "            word = line.strip()\n",
    "            if (word == ''):\n",
    "                out_file.write(\"\\n\")\n",
    "            else:\n",
    "                out_file.write(\"{} {}\\n\".format(word, predict_tag_optimized(data_cleaned, word)))\n",
    "    print(\"Finished: {}\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: EN\n",
      "\n",
      "#Entity in gold data: 226\n",
      "#Entity in prediction: 1201\n",
      "\n",
      "#Correct Entity : 165\n",
      "Entity  precision: 0.1374\n",
      "Entity  recall: 0.7301\n",
      "Entity  F: 0.2313\n",
      "\n",
      "#Correct Sentiment : 71\n",
      "Sentiment  precision: 0.0591\n",
      "Sentiment  recall: 0.3142\n",
      "Sentiment  F: 0.0995\n",
      "\n",
      "----------------------\n",
      "Language: SG\n",
      "\n",
      "#Entity in gold data: 1382\n",
      "#Entity in prediction: 6599\n",
      "\n",
      "#Correct Entity : 794\n",
      "Entity  precision: 0.1203\n",
      "Entity  recall: 0.5745\n",
      "Entity  F: 0.1990\n",
      "\n",
      "#Correct Sentiment : 315\n",
      "Sentiment  precision: 0.0477\n",
      "Sentiment  recall: 0.2279\n",
      "Sentiment  F: 0.0789\n",
      "\n",
      "----------------------\n",
      "Language: CN\n",
      "\n",
      "#Entity in gold data: 362\n",
      "#Entity in prediction: 3318\n",
      "\n",
      "#Correct Entity : 183\n",
      "Entity  precision: 0.0552\n",
      "Entity  recall: 0.5055\n",
      "Entity  F: 0.0995\n",
      "\n",
      "#Correct Sentiment : 57\n",
      "Sentiment  precision: 0.0172\n",
      "Sentiment  recall: 0.1575\n",
      "Sentiment  F: 0.0310\n",
      "\n",
      "----------------------\n",
      "Language: FR\n",
      "\n",
      "#Entity in gold data: 223\n",
      "#Entity in prediction: 1149\n",
      "\n",
      "#Correct Entity : 182\n",
      "Entity  precision: 0.1584\n",
      "Entity  recall: 0.8161\n",
      "Entity  F: 0.2653\n",
      "\n",
      "#Correct Sentiment : 68\n",
      "Sentiment  precision: 0.0592\n",
      "Sentiment  recall: 0.3049\n",
      "Sentiment  F: 0.0991\n",
      "\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "for l in languages:\n",
    "    output = os.popen(\"python EvalScript/evalResult.py {0}/dev.out {0}/dev.p2.out\".format(l)).read()\n",
    "    print(\"Language: {}\".format(l))\n",
    "    print(output)\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimates the transition parameters from the training set using MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "def transition_params(ordered_labels_list: list):\n",
    "    count = {}\n",
    "    count_given = {} # 2 layer dictionary depth-0 key is the (i-1)-label, depth-1 key is the i-label\n",
    "    \n",
    "    # count frequency of all label and combinations of 2 labels in the dataset\n",
    "    for idx, label in enumerate(ordered_labels_list):\n",
    "        if label not in count:\n",
    "            count[label] = 1\n",
    "            count_given[label] = {}\n",
    "            if idx < len(ordered_labels_list) - 1:\n",
    "                next_label = ordered_labels_list[idx + 1]\n",
    "                count_given[label][next_label] = 1\n",
    "        else:\n",
    "            count[label] += 1\n",
    "            if idx < len(ordered_labels_list) - 1:\n",
    "                next_label = ordered_labels_list[idx + 1]\n",
    "                if next_label not in count_given[label]:\n",
    "                    count_given[label][next_label] = 1\n",
    "                else:\n",
    "                    count_given[label][next_label] += 1\n",
    "    \n",
    "    # calculate trans_params\n",
    "    trans_params = copy.deepcopy(count_given)\n",
    "    for given_label in trans_params:\n",
    "        for label in trans_params[given_label]:\n",
    "            trans_params[given_label][label] /= count[given_label]\n",
    "            \n",
    "    return trans_params\n",
    "\n",
    "def specific_transition_params(ordered_labels_list: list, y: str, y_given: str):\n",
    "    trans_params = transition_params(ordered_labels_list)\n",
    "    if y not in trans_params:\n",
    "        return 0;\n",
    "    elif y_given not in trans_params[y]:\n",
    "        return 0;\n",
    "    else:\n",
    "        return trans_params[y_given][y]\n",
    "    \n",
    "specific_transition_params(['a', 'b', 'b', 'c', 'b', 'a', 'd', 'h', 'b'], 'b', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viterbi algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 0))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hey': {'I': 1.0}, 'I': {'was': 0.14285714285714285, 'met': 0.14285714285714285, 'hope': 0.14285714285714285, 'never': 0.14285714285714285, 'know': 0.14285714285714285, \"can't\": 0.2857142857142857}, 'was': {'doing': 0.5, 'nice': 0.5}, 'doing': {'just': 1.0}, 'just': {'fine': 1.0}, 'fine': {'before': 1.0}, 'before': {'I': 1.0}, 'met': {'you': 1.0}, 'you': {'Drink': 0.5, 'tell': 0.5}, 'Drink': {'too': 1.0}, 'too': {'much': 1.0}, 'much': {'and': 1.0}, 'and': {\"that's\": 1.0}, \"that's\": {'an': 1.0}, 'an': {'issue': 1.0}, 'issue': {'But': 1.0}, 'But': {\"I'm\": 0.5, 'I': 0.5}, \"I'm\": {'okay': 1.0}, 'okay': {'Hey,': 1.0}, 'Hey,': {'you': 1.0}, 'tell': {'your': 1.0}, 'your': {'friends': 0.5, 'heart': 0.5}, 'friends': {'it': 1.0}, 'it': {'was': 0.5, 'breaks': 0.5}, 'nice': {'to': 1.0}, 'to': {'meet': 0.5, 'the': 0.5}, 'meet': {'them': 1.0}, 'them': {'But': 0.5, 'Again': 0.5}, 'hope': {'I': 1.0}, 'never': {'see': 1.0}, 'see': {'them': 1.0}, 'Again': {'I': 1.0}, 'know': {'it': 1.0}, 'breaks': {'your': 1.0}, 'heart': {'Moved': 1.0}, 'Moved': {'to': 1.0}, 'the': {'city': 1.0}, 'city': {'in': 1.0}, 'in': {'a': 1.0}, 'a': {'broke-down': 0.5, 'hotel': 0.5}, 'broke-down': {'car': 1.0}, 'car': {'And': 1.0}, 'And': {'four': 0.5, 'I,': 0.5}, 'four': {'years,': 1.0}, 'years,': {'no': 1.0}, 'no': {'calls': 1.0}, 'calls': {'Now': 1.0}, 'Now': {\"you're\": 1.0}, \"you're\": {'looking': 1.0}, 'looking': {'pretty': 1.0}, 'pretty': {'in': 1.0}, 'hotel': {'bar': 1.0}, 'bar': {'And': 1.0}, 'I,': {'I,': 0.75, 'I': 0.25}, \"can't\": {'stop': 1.0}, 'stop': {'No,': 0.5}, 'No,': {'I,': 1.0}}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3fb4fca9a50d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mAnd\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m \u001b[0mcan\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mNo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m \u001b[0mcan\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \"\"\")\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-3fb4fca9a50d>\u001b[0m in \u001b[0;36mviterbi\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m          \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m viterbi(\"\"\"\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "def viterbi(sentence: str):\n",
    "    sentence = sentence.split()\n",
    "    # sentence has both START and STOP words\n",
    "    cache = {}\n",
    "    y_predicted = []\n",
    "    trans_params = transition_params(sentence)\n",
    "    print(trans_params)\n",
    "    for k in range(len(sentence)-1):\n",
    "         pass\n",
    "    return result\n",
    "\n",
    "viterbi(\"\"\"\n",
    "Hey I was doing just fine before I met you\n",
    "Drink too much and that\\'s an issue\n",
    "But I\\'m okay\n",
    "Hey, you tell your friends it was nice to meet them\n",
    "But I hope I never see them\n",
    "Again\n",
    "I know it breaks your heart\n",
    "Moved to the city in a broke-down car\n",
    "And four years, no calls\n",
    "Now you're looking pretty in a hotel bar\n",
    "And I, I, I, I, I can't stop\n",
    "No, I, I, I, I, I can't stop\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
