{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Estimate the emission parameters from the training set using MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "class Emission:\n",
    "    def __init__(self):\n",
    "        self.emission_p = {}\n",
    "        self.y_count = {}\n",
    "        self.y_labels = []\n",
    "        self.x_given_y_count = {}\n",
    "        self.tokens_list = []\n",
    "        self.special_token = '#UNK#'\n",
    "    \n",
    "    def clean_data(self, k = 1):\n",
    "        token_freq = {}\n",
    "        for token in self.tokens_list:\n",
    "            if token[0] not in token_freq: \n",
    "                token_freq[token[0]] = 1\n",
    "            else:\n",
    "                token_freq[token[0]] += 1\n",
    "        for i in range(len(self.tokens_list)):\n",
    "            if token_freq[self.tokens_list[i][0]] < k:\n",
    "                self.tokens_list[i][0] = self.special_token\n",
    "        return self.tokens_list\n",
    "    \n",
    "    def train(self, tokens_list: list, k = 1, special_token = '#UNK#'):\n",
    "        self.tokens_list = tokens_list\n",
    "        self.special_token = special_token\n",
    "        self.clean_data(k)\n",
    "        self.y_count = {}\n",
    "        self.x_given_y_count = {} \n",
    "\n",
    "        for token in tokens_list:\n",
    "            if token[1] not in self.y_count:\n",
    "                self.y_count[token[1]] = 1\n",
    "                self.x_given_y_count[token[1]] = {}\n",
    "                self.x_given_y_count[token[1]][token[0]] = 1\n",
    "            else:\n",
    "                self.y_count[token[1]] += 1\n",
    "                if token[0] not in self.x_given_y_count[token[1]]:\n",
    "                    self.x_given_y_count[token[1]][token[0]] = 1\n",
    "                else:\n",
    "                    self.x_given_y_count[token[1]][token[0]] += 1\n",
    "\n",
    "        # calculate emission params\n",
    "        self.emission_p = copy.deepcopy(self.x_given_y_count)\n",
    "        for label in self.emission_p:\n",
    "            for word in self.emission_p[label]:\n",
    "                self.emission_p[label][word] = float(self.x_given_y_count[label][word]) / self.y_count[label]\n",
    "        self.y_labels = list(self.emission_p.keys())\n",
    "        return self.emission_p\n",
    "    \n",
    "    def predict(self, y: str, x: str):\n",
    "        x_inside = False\n",
    "        for token in self.tokens_list:\n",
    "            if token[0] == x:\n",
    "                x_inside = True\n",
    "                break\n",
    "\n",
    "        if not x_inside:\n",
    "            x = self.special_token\n",
    "\n",
    "        if x not in self.emission_p[y]:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.emission_p[y][x]\n",
    "        \n",
    "    def predict_tag(self, x: str):\n",
    "        score = 0.0\n",
    "        y_tag = None\n",
    "        for y in self.y_labels:\n",
    "            y_score = self.predict(y, x)\n",
    "            if y_score > score:\n",
    "                y_tag = y\n",
    "                score = y_score\n",
    "        return y_tag\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': {'a': 0.3333333333333333, 'b': 0.3333333333333333, 'c': 0.3333333333333333}, 'I': {'a': 1.0}}\n",
      "0.3333333333333333\n",
      "['O', 'I']\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "# Example ussage\n",
    "import os\n",
    "\n",
    "data = [['a', 'O'], ['b', 'O'], ['a', 'I'], ['c', 'O']]\n",
    "model = Emission()\n",
    "print(model.train(tokens_list=data))\n",
    "print(model.predict(y='O', x='b'))\n",
    "print(model.y_labels)\n",
    "print(model.predict_tag('a'))\n",
    "\n",
    "# with open('./EN/train') as train_file:\n",
    "#     read_data = train_file.read()\n",
    "#     read_data = os.linesep.join([s for s in read_data.splitlines() if s])\n",
    "#     data = list(map(lambda x: x.split(' '),read_data.split('\\n')))\n",
    "# emission_params(tokens_list=data, y='O', x='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify training set to replace words that appear less than k times with special token. Apply this to the emission parameters prediction function with k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "data = [['a', 'O'], ['b', 'O'], ['a', 'I'], ['c', 'O'], ['a', 'O']]\n",
    "model = Emission()\n",
    "model.train(tokens_list=data, k = 3)\n",
    "model.predict(y='O', x='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis system that produces the tag for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish training for EN\n",
      "Finished: EN\n",
      "Finish training for SG\n",
      "Finished: SG\n",
      "Finish training for CN\n",
      "Finished: CN\n",
      "Finish training for FR\n",
      "Finished: FR\n"
     ]
    }
   ],
   "source": [
    "languages = ['EN', 'SG', 'CN', 'FR']\n",
    "\n",
    "for l in languages:\n",
    "    model = Emission()\n",
    "    with open(\"./{}/train\".format(l)) as train_file:\n",
    "        read_data = train_file.read()\n",
    "        read_data = os.linesep.join([s for s in read_data.splitlines() if s])\n",
    "        data = list(map(lambda x: x.split(' '),read_data.split('\\n')))\n",
    "        model.train(tokens_list=data, k=3)\n",
    "    \n",
    "    print(\"Finish training for {}\".format(l))\n",
    "\n",
    "    with open(\"./{}/dev.in\".format(l)) as in_file, open(\"./{}/dev.p2.out\".format(l), 'w+') as out_file:\n",
    "        for line in in_file:\n",
    "            word = line.strip()\n",
    "            if (word == ''):\n",
    "                out_file.write(\"\\n\")\n",
    "            else:\n",
    "                out_file.write(\"{} {}\\n\".format(word, model.predict_tag(word)))\n",
    "    print(\"Finished: {}\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: EN\n",
      "\n",
      "#Entity in gold data: 226\n",
      "#Entity in prediction: 1201\n",
      "\n",
      "#Correct Entity : 165\n",
      "Entity  precision: 0.1374\n",
      "Entity  recall: 0.7301\n",
      "Entity  F: 0.2313\n",
      "\n",
      "#Correct Sentiment : 71\n",
      "Sentiment  precision: 0.0591\n",
      "Sentiment  recall: 0.3142\n",
      "Sentiment  F: 0.0995\n",
      "\n",
      "----------------------\n",
      "Language: SG\n",
      "\n",
      "#Entity in gold data: 1382\n",
      "#Entity in prediction: 6542\n",
      "\n",
      "#Correct Entity : 780\n",
      "Entity  precision: 0.1192\n",
      "Entity  recall: 0.5644\n",
      "Entity  F: 0.1969\n",
      "\n",
      "#Correct Sentiment : 311\n",
      "Sentiment  precision: 0.0475\n",
      "Sentiment  recall: 0.2250\n",
      "Sentiment  F: 0.0785\n",
      "\n",
      "----------------------\n",
      "Language: CN\n",
      "\n",
      "#Entity in gold data: 362\n",
      "#Entity in prediction: 3318\n",
      "\n",
      "#Correct Entity : 183\n",
      "Entity  precision: 0.0552\n",
      "Entity  recall: 0.5055\n",
      "Entity  F: 0.0995\n",
      "\n",
      "#Correct Sentiment : 57\n",
      "Sentiment  precision: 0.0172\n",
      "Sentiment  recall: 0.1575\n",
      "Sentiment  F: 0.0310\n",
      "\n",
      "----------------------\n",
      "Language: FR\n",
      "\n",
      "#Entity in gold data: 223\n",
      "#Entity in prediction: 1149\n",
      "\n",
      "#Correct Entity : 182\n",
      "Entity  precision: 0.1584\n",
      "Entity  recall: 0.8161\n",
      "Entity  F: 0.2653\n",
      "\n",
      "#Correct Sentiment : 68\n",
      "Sentiment  precision: 0.0592\n",
      "Sentiment  recall: 0.3049\n",
      "Sentiment  F: 0.0991\n",
      "\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "for l in languages:\n",
    "    output = os.popen(\"python3 EvalScript/evalResult.py {0}/dev.out {0}/dev.p2.out\".format(l)).read()\n",
    "    print(\"Language: {}\".format(l))\n",
    "    print(output)\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimates the transition parameters from the training set using MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "def transition_params(ordered_labels_list: list):\n",
    "    count = {}\n",
    "    count_given = {} # 2 layer dictionary depth-0 key is the (i-1)-label, depth-1 key is the i-label\n",
    "    \n",
    "    # count frequency of all label and combinations of 2 labels in the dataset\n",
    "    for idx, label in enumerate(ordered_labels_list):\n",
    "        if label not in count:\n",
    "            count[label] = 1\n",
    "            count_given[label] = {}\n",
    "            if idx < len(ordered_labels_list) - 1:\n",
    "                next_label = ordered_labels_list[idx + 1]\n",
    "                count_given[label][next_label] = 1\n",
    "        else:\n",
    "            count[label] += 1\n",
    "            if idx < len(ordered_labels_list) - 1:\n",
    "                next_label = ordered_labels_list[idx + 1]\n",
    "                if next_label not in count_given[label]:\n",
    "                    count_given[label][next_label] = 1\n",
    "                else:\n",
    "                    count_given[label][next_label] += 1\n",
    "    \n",
    "    # calculate trans_params\n",
    "    trans_params = copy.deepcopy(count_given)\n",
    "    for given_label in trans_params:\n",
    "        for label in trans_params[given_label]:\n",
    "            trans_params[given_label][label] /= count[given_label]\n",
    "            \n",
    "    return trans_params\n",
    "\n",
    "def specific_transition_params(ordered_labels_list: list, y: str, y_given: str):\n",
    "    trans_params = transition_params(ordered_labels_list)\n",
    "    if y not in trans_params:\n",
    "        return 0;\n",
    "    elif y_given not in trans_params[y]:\n",
    "        return 0;\n",
    "    else:\n",
    "        return trans_params[y_given][y]\n",
    "    \n",
    "specific_transition_params(['a', 'b', 'b', 'c', 'b', 'a', 'd', 'h', 'b'], 'b', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viterbi algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Healthy', 'Healthy', 'Fever'], 0.00034279199999999997)\n"
     ]
    }
   ],
   "source": [
    "def viterbi(sentence: str, labels: list, trans_p: dict, emission_p: dict):\n",
    "    observed_words = sentence.split()\n",
    "    cache = [{}]\n",
    "    \n",
    "    # handle first layer\n",
    "    for l in labels:\n",
    "        trans_param = trans_p['START'][l] if l in trans_p['START'] else 0\n",
    "        emission_param = emission_p[l][observed_words[0]] if observed_words[0] in emission_p[l] else 0\n",
    "        cache[0][l] = {\"chance\": trans_param * emission_param, \"prev\": None}\n",
    "    \n",
    "    # handle middle layers\n",
    "    for i in range(1, len(observed_words)):\n",
    "        cache.append({})\n",
    "        max_trans_prob = 0\n",
    "        max_prev_l = None\n",
    "        for l in labels:\n",
    "            for prev_l in labels:\n",
    "                trans_param = trans_p[prev_l][l] if l in trans_p[prev_l] else 0\n",
    "                trans_prob = cache[i-1][l]['chance'] * trans_param\n",
    "                if trans_prob > max_trans_prob:\n",
    "                    max_trans_prob = trans_prob\n",
    "                    max_prev_l = prev_l\n",
    "            \n",
    "            emission_param = emission_p[l][observed_words[i]] if observed_words[i] in emission_p[l] else 0\n",
    "            cache[i][l] = {'chance': max_trans_prob * emission_param, 'prev': max_prev_l}\n",
    "            \n",
    "    # handle the end layer       \n",
    "    cache.append({})\n",
    "    max_end_prob = 0\n",
    "    max_end_l = None\n",
    "    for l in labels:\n",
    "        trans_param = trans_p[l]['STOP'] if 'STOP' in trans_p[l] else 0\n",
    "        end_prob = cache[len(observed_words) - 1][l]['chance'] * trans_param\n",
    "        if end_prob > max_end_prob:\n",
    "            max_end_prob = end_prob\n",
    "            max_end_l = l\n",
    "    cache[len(observed_words)]['STOP'] = {'chance': max_end_prob, 'prev': max_end_l}\n",
    "    \n",
    "    # backtrack for optimal path\n",
    "    optimal_prob = cache[len(observed_words)]['STOP']['chance']\n",
    "    previous_l = cache[len(observed_words)]['STOP']['prev']\n",
    "    optimal = [previous_l]\n",
    "    for i in range(len(observed_words) - 1, 0, -1):\n",
    "        optimal.insert(0, cache[i][previous_l]['prev'])\n",
    "        previous = cache[i][previous_l]['prev']\n",
    "    \n",
    "    return (optimal, optimal_prob)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "print(viterbi(\n",
    "    'normal cold dizzy', \n",
    "    ['Healthy', 'Fever'],\n",
    "    {\n",
    "        'START': {'Healthy': 0.6, 'Fever': 0.4},\n",
    "        'Healthy': {'Healthy': 0.69, 'Fever': 0.3, 'STOP': 0.01},\n",
    "        'Fever': {'Healthy': 0.4, 'Fever': 0.59, 'STOP': 0.01}\n",
    "    },\n",
    "    {\n",
    "        'Healthy' : {'normal': 0.5, 'cold': 0.4, 'dizzy': 0.1},\n",
    "        'Fever' : {'normal': 0.1, 'cold': 0.3, 'dizzy': 0.6}\n",
    "    }\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward-backward algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'Fever': 0.04000000000000001, 'Healthy': 0.3},\n",
       "  {'Fever': 0.03408, 'Healthy': 0.0892},\n",
       "  {'Fever': 0.028120319999999997, 'Healthy': 0.007518}],\n",
       " [{'Fever': 0.00109578, 'Healthy': 0.0010418399999999998},\n",
       "  {'Fever': 0.00394, 'Healthy': 0.00249},\n",
       "  {'Fever': 0.01, 'Healthy': 0.01}])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "def forward_backward(sentence: str, labels: list, trans_p: dict, emission_p: dict):\n",
    "    observed_words = sentence.split()\n",
    "    \n",
    "    # forward part\n",
    "    forward = []\n",
    "    prev_forward = {}\n",
    "    for i, word in enumerate(observed_words):\n",
    "        curr_forward = {}\n",
    "        for l in labels:\n",
    "            prev_f_sum = 0\n",
    "            if i == 0:\n",
    "                trans_prob = trans_p['START'][l] if l in trans_p['START'] else 0\n",
    "                prev_f_sum = trans_prob\n",
    "            else:\n",
    "                for prev_l in labels:\n",
    "                    trans_prob = trans_p[prev_l][l] if l in trans_p[prev_l] else 0\n",
    "                    prev_f_sum += prev_forward[prev_l] * trans_prob\n",
    "            \n",
    "            curr_forward[l] = emission_p[l][word] * prev_f_sum\n",
    "        \n",
    "        forward.append(curr_forward)\n",
    "        prev_forward = copy.deepcopy(curr_forward)\n",
    "    \n",
    "    forward_prob = 0\n",
    "    for l in labels:\n",
    "        trans_prob = trans_p[l]['STOP'] if 'STOP' in trans_p[l] else 0\n",
    "        forward_prob += curr_forward[l] * trans_prob\n",
    "    \n",
    "    # backward part\n",
    "    backward = []\n",
    "    prev_backward = {}\n",
    "    for i, word in enumerate((observed_words[1:] + [None])[::-1]):\n",
    "        curr_backward = {}\n",
    "        for l in labels:\n",
    "            curr_backward[l] = 0\n",
    "            if i == 0:\n",
    "                trans_prob = trans_p[l]['STOP'] if 'STOP' in trans_p[l] else 0\n",
    "                curr_backward[l] = trans_prob\n",
    "            else:\n",
    "                for next_l in labels:\n",
    "                    trans_prob = trans_p[l][next_l] if next_l in trans_p[l] else 0\n",
    "                    emm_prob = emission_p[next_l][word] if word in emission_p[next_l] else 0\n",
    "                    curr_backward[l] += trans_prob * emm_prob * prev_backward[next_l]\n",
    "        \n",
    "        backward.insert(0, curr_backward)\n",
    "        prev_backward = copy.deepcopy(curr_backward)\n",
    "    \n",
    "    backward_prob = 0\n",
    "    for l in labels:\n",
    "        trans_prob = trans_p['START'][l] if l in trans_p['START'] else 0\n",
    "        emm_prob = emission_p[l][observed_words[0]] \n",
    "        backward_prob += trans_prob * emm_prob * curr_backward[l]\n",
    "        \n",
    "    return forward, backward\n",
    "\n",
    "forward_backward(\n",
    "    'normal cold dizzy', \n",
    "    ['Healthy', 'Fever'],\n",
    "    {\n",
    "        'START': {'Healthy': 0.6, 'Fever': 0.4},\n",
    "        'Healthy': {'Healthy': 0.69, 'Fever': 0.3, 'STOP': 0.01},\n",
    "        'Fever': {'Healthy': 0.4, 'Fever': 0.59, 'STOP': 0.01}\n",
    "    },\n",
    "    {\n",
    "        'Healthy' : {'normal': 0.5, 'cold': 0.4, 'dizzy': 0.1},\n",
    "        'Fever' : {'normal': 0.1, 'cold': 0.3, 'dizzy': 0.6}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max-Marginal Decoding with Forward-backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Healthy', 'Healthy', 'Fever']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max_marginal(sentence: str, labels: list, trans_p: dict, emission_p: dict):\n",
    "    forward_p, backward_p = forward_backward(sentence, labels, trans_p, emission_p)\n",
    "    predictions = []\n",
    "    for i in range(len(forward_p)):\n",
    "        product_p = {l: forward_p[i][l] * backward_p[i][l] for l in labels}\n",
    "        predictions.append(max(product_p, key=product_p.get))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "max_marginal(\n",
    "    'normal cold dizzy', \n",
    "    ['Healthy', 'Fever'],\n",
    "    {\n",
    "        'START': {'Healthy': 0.6, 'Fever': 0.4},\n",
    "        'Healthy': {'Healthy': 0.69, 'Fever': 0.3, 'STOP': 0.01},\n",
    "        'Fever': {'Healthy': 0.4, 'Fever': 0.59, 'STOP': 0.01}\n",
    "    },\n",
    "    {\n",
    "        'Healthy' : {'normal': 0.5, 'cold': 0.4, 'dizzy': 0.1},\n",
    "        'Fever' : {'normal': 0.1, 'cold': 0.3, 'dizzy': 0.6}\n",
    "    }\n",
    ")      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
