{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hmm import SUTDHMM\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Estimate the emission parameters from the training set using MLE. Our approach was to count all the occurences of word give a certain label and the occurences of labels when we do ```load_data```. Emission parameters are actually calculated inside ```calculate_emission``` method (which is called inside ```train``` method). Please investigate [hmm.py](./hmm.py) for further understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish training for EN\n",
      "Finished: EN\n",
      "Language: EN\n",
      "\n",
      "#Entity in gold data: 226\n",
      "#Entity in prediction: 706\n",
      "\n",
      "#Correct Entity : 133\n",
      "Entity  precision: 0.1884\n",
      "Entity  recall: 0.5885\n",
      "Entity  F: 0.2854\n",
      "\n",
      "#Correct Sentiment : 45\n",
      "Sentiment  precision: 0.0637\n",
      "Sentiment  recall: 0.1991\n",
      "Sentiment  F: 0.0966\n",
      "\n",
      "----------------------\n",
      "Finish training for SG\n",
      "Finished: SG\n",
      "Language: SG\n",
      "\n",
      "#Entity in gold data: 1382\n",
      "#Entity in prediction: 2764\n",
      "\n",
      "#Correct Entity : 511\n",
      "Entity  precision: 0.1849\n",
      "Entity  recall: 0.3698\n",
      "Entity  F: 0.2465\n",
      "\n",
      "#Correct Sentiment : 272\n",
      "Sentiment  precision: 0.0984\n",
      "Sentiment  recall: 0.1968\n",
      "Sentiment  F: 0.1312\n",
      "\n",
      "----------------------\n",
      "Finish training for CN\n",
      "Finished: CN\n",
      "Language: CN\n",
      "\n",
      "#Entity in gold data: 362\n",
      "#Entity in prediction: 1688\n",
      "\n",
      "#Correct Entity : 114\n",
      "Entity  precision: 0.0675\n",
      "Entity  recall: 0.3149\n",
      "Entity  F: 0.1112\n",
      "\n",
      "#Correct Sentiment : 71\n",
      "Sentiment  precision: 0.0421\n",
      "Sentiment  recall: 0.1961\n",
      "Sentiment  F: 0.0693\n",
      "\n",
      "----------------------\n",
      "Finish training for FR\n",
      "Finished: FR\n",
      "Language: FR\n",
      "\n",
      "#Entity in gold data: 223\n",
      "#Entity in prediction: 691\n",
      "\n",
      "#Correct Entity : 174\n",
      "Entity  precision: 0.2518\n",
      "Entity  recall: 0.7803\n",
      "Entity  F: 0.3807\n",
      "\n",
      "#Correct Sentiment : 65\n",
      "Sentiment  precision: 0.0941\n",
      "Sentiment  recall: 0.2915\n",
      "Sentiment  F: 0.1422\n",
      "\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "from hmm import SUTDHMM\n",
    "languages = ['EN', 'SG', 'CN', 'FR']\n",
    "\n",
    "for l in languages:\n",
    "    model = SUTDHMM()\n",
    "    model.train(input_filename='./{}/train'.format(l))\n",
    "    print(\"Finish training for {}\".format(l))\n",
    "\n",
    "    with open(\"./{}/dev.in\".format(l)) as in_file, open(\"./{}/dev.p2.out\".format(l), 'w+') as out_file:\n",
    "        for line in in_file:\n",
    "            word = line.strip()\n",
    "            if (word == ''):\n",
    "                out_file.write(\"\\n\")\n",
    "            else:\n",
    "                out_file.write(\"{} {}\\n\".format(word, model.predict_label_using_emission(word)))\n",
    "    print(\"Finished: {}\".format(l))\n",
    "    \n",
    "    output = os.popen(\"python3 EvalScript/evalResult.py {0}/dev.out {0}/dev.p2.out\".format(l)).read()\n",
    "    print(\"Language: {}\".format(l))\n",
    "    print(output)\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "We calculate the transition parameters the same way as emission paramters are calculated. We counted all the occurences of the different transitions and occurences of the different labels at the ```load_data``` step and make the calculation in ```calculate_transition``` method (which is in turn called inside ```train``` method). Please investigate [hmm.py](./hmm.py) for further understanding.\n",
    "\n",
    "Our Viterbi algorithm is implemented as the method ```viterbi``` of the main class ```SUTDHMM```, which makes use of the previously calculated emission and transition parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish training for EN\n",
      "----------Viterbi for EN------------\n",
      "Viterbi Finished: EN\n",
      "Language: EN\n",
      "\n",
      "#Entity in gold data: 226\n",
      "#Entity in prediction: 116\n",
      "\n",
      "#Correct Entity : 74\n",
      "Entity  precision: 0.6379\n",
      "Entity  recall: 0.3274\n",
      "Entity  F: 0.4327\n",
      "\n",
      "#Correct Sentiment : 51\n",
      "Sentiment  precision: 0.4397\n",
      "Sentiment  recall: 0.2257\n",
      "Sentiment  F: 0.2982\n",
      "\n",
      "Finish training for SG\n",
      "----------Viterbi for SG------------\n",
      "Viterbi Finished: SG\n",
      "Language: SG\n",
      "\n",
      "#Entity in gold data: 1382\n",
      "#Entity in prediction: 499\n",
      "\n",
      "#Correct Entity : 228\n",
      "Entity  precision: 0.4569\n",
      "Entity  recall: 0.1650\n",
      "Entity  F: 0.2424\n",
      "\n",
      "#Correct Sentiment : 152\n",
      "Sentiment  precision: 0.3046\n",
      "Sentiment  recall: 0.1100\n",
      "Sentiment  F: 0.1616\n",
      "\n",
      "Finish training for CN\n",
      "----------Viterbi for CN------------\n",
      "Viterbi Finished: CN\n",
      "Language: CN\n",
      "\n",
      "#Entity in gold data: 362\n",
      "#Entity in prediction: 173\n",
      "\n",
      "#Correct Entity : 28\n",
      "Entity  precision: 0.1618\n",
      "Entity  recall: 0.0773\n",
      "Entity  F: 0.1047\n",
      "\n",
      "#Correct Sentiment : 17\n",
      "Sentiment  precision: 0.0983\n",
      "Sentiment  recall: 0.0470\n",
      "Sentiment  F: 0.0636\n",
      "\n",
      "Finish training for FR\n",
      "----------Viterbi for FR------------\n",
      "Viterbi Finished: FR\n",
      "Language: FR\n",
      "\n",
      "#Entity in gold data: 223\n",
      "#Entity in prediction: 115\n",
      "\n",
      "#Correct Entity : 79\n",
      "Entity  precision: 0.6870\n",
      "Entity  recall: 0.3543\n",
      "Entity  F: 0.4675\n",
      "\n",
      "#Correct Sentiment : 51\n",
      "Sentiment  precision: 0.4435\n",
      "Sentiment  recall: 0.2287\n",
      "Sentiment  F: 0.3018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "languages = ['EN', 'SG', 'CN', 'FR']\n",
    "\n",
    "for l in languages:\n",
    "    model = SUTDHMM()\n",
    "    model.train(input_filename='./{}/train'.format(l))\n",
    "\n",
    "    print(\"Finish training for {}\".format(l))\n",
    "\n",
    "    print(\"----------Viterbi for {0}------------\".format(l))\n",
    "    with open(\"./{}/dev.in\".format(l)) as in_file, open(\"./{}/dev.p3.out\".format(l), 'w+') as out_file:\n",
    "        read_data = in_file.read()\n",
    "        sentences = list(filter(lambda x: len(x) > 0, read_data.split('\\n\\n')))\n",
    "        sentences = list(map(lambda x: ' '.join(x.split('\\n')), sentences))\n",
    "        for sentence in sentences:\n",
    "            sentence_labels, chance = model.viterbi(sentence)\n",
    "            for idx, word in enumerate(sentence.split()):\n",
    "                out_file.write(\"{} {}\\n\".format(word, sentence_labels[idx]))\n",
    "            out_file.write('\\n')\n",
    "        out_file.close()\n",
    "        in_file.close()\n",
    "\n",
    "    print(\"Viterbi Finished: {}\".format(l))\n",
    "    \n",
    "    output = os.popen(\n",
    "        \"python3 EvalScript/evalResult.py {0}/dev.out {0}/dev.p3.out\".format(l)).read()\n",
    "    print(\"Language: {}\".format(l))\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "\n",
    "The max marginal approach attempts to find the optimal path with the following approach:\n",
    "\n",
    "$ y_i^* = \\arg\\max_{y_i} \\{p (y_i \\mid x_1, x_2,...,x_n; \\theta)\\} $\n",
    "\n",
    "The conditional probability of a state $u$ occuring for $y_i$ is given as follows:\n",
    "\n",
    "$ p (y_i = u \\mid x_1, x_2,...,x_n; \\theta) = \\frac {p(x_1, x_2,...x_{i-1},y_i=u,x_i,...,x_n; \\theta)}{p(x_1,...x_n; \\theta)} $\n",
    "\n",
    "As $x_i,...x_n$ are independent of $x_1,...x_{i-1}$ once $y_i$ is known in a Hidden Markov Model, the conditional probability could be written as such:\n",
    "\n",
    "$ p (y_i = u \\mid x_1, x_2,...,x_n; \\theta) =\\frac {p(x_1, x_2,...x_{i-1},y_i=u; \\theta)p(x_i,...,x_n \\mid y_i = u;\\theta)}{p(x_1,...x_n; \\theta)} $\n",
    "\n",
    "$\\qquad\\qquad\\qquad\\qquad\\quad=\\frac {\\alpha_u(i)\\beta_u(i)}{\\sum_{v}\\alpha_v(j)\\beta_v(j)} , \\quad \\text{where} \\quad j \\in (1,2,...n) $\n",
    "\n",
    "Thus, the following result could be obtained to indicate the optimum state for each $y$\n",
    "\n",
    "$ y_i^* = \\arg\\max_{u} \\frac {\\alpha_u(i)\\beta_u(i)}{\\sum_{v}\\alpha_v(j)\\beta_v(j)} =  \\arg\\max_{u} \\alpha_u(i)\\beta_u(i) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish training for EN\n",
      "----------Max Marginal for EN------------\n",
      "Max Marginal Finished: EN\n",
      "Language: EN\n",
      "\n",
      "#Entity in gold data: 226\n",
      "#Entity in prediction: 113\n",
      "\n",
      "#Correct Entity : 71\n",
      "Entity  precision: 0.6283\n",
      "Entity  recall: 0.3142\n",
      "Entity  F: 0.4189\n",
      "\n",
      "#Correct Sentiment : 45\n",
      "Sentiment  precision: 0.3982\n",
      "Sentiment  recall: 0.1991\n",
      "Sentiment  F: 0.2655\n",
      "\n",
      "Finish training for SG\n",
      "----------Max Marginal for SG------------\n",
      "Max Marginal Finished: SG\n",
      "Language: SG\n",
      "\n",
      "#Entity in gold data: 1382\n",
      "#Entity in prediction: 464\n",
      "\n",
      "#Correct Entity : 258\n",
      "Entity  precision: 0.5560\n",
      "Entity  recall: 0.1867\n",
      "Entity  F: 0.2795\n",
      "\n",
      "#Correct Sentiment : 178\n",
      "Sentiment  precision: 0.3836\n",
      "Sentiment  recall: 0.1288\n",
      "Sentiment  F: 0.1928\n",
      "\n",
      "Finish training for CN\n",
      "----------Max Marginal for CN------------\n",
      "Max Marginal Finished: CN\n",
      "Language: CN\n",
      "\n",
      "#Entity in gold data: 362\n",
      "#Entity in prediction: 187\n",
      "\n",
      "#Correct Entity : 71\n",
      "Entity  precision: 0.3797\n",
      "Entity  recall: 0.1961\n",
      "Entity  F: 0.2587\n",
      "\n",
      "#Correct Sentiment : 52\n",
      "Sentiment  precision: 0.2781\n",
      "Sentiment  recall: 0.1436\n",
      "Sentiment  F: 0.1894\n",
      "\n",
      "Finish training for FR\n",
      "----------Max Marginal for FR------------\n",
      "Max Marginal Finished: FR\n",
      "Language: FR\n",
      "\n",
      "#Entity in gold data: 223\n",
      "#Entity in prediction: 91\n",
      "\n",
      "#Correct Entity : 59\n",
      "Entity  precision: 0.6484\n",
      "Entity  recall: 0.2646\n",
      "Entity  F: 0.3758\n",
      "\n",
      "#Correct Sentiment : 33\n",
      "Sentiment  precision: 0.3626\n",
      "Sentiment  recall: 0.1480\n",
      "Sentiment  F: 0.2102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "languages = ['EN', 'SG', 'CN', 'FR']\n",
    "\n",
    "for l in languages:\n",
    "    model = SUTDHMM()\n",
    "    model.train(input_filename='./{}/train'.format(l))\n",
    "\n",
    "    print(\"Finish training for {}\".format(l))\n",
    "\n",
    "    print(\"----------Max Marginal for {0}------------\".format(l))\n",
    "    with open(\"./{}/dev.in\".format(l)) as in_file, open(\"./{}/dev.p4.out\".format(l), 'w+') as out_file:\n",
    "        read_data = in_file.read()\n",
    "        sentences = list(filter(lambda x: len(x) > 0, read_data.split('\\n\\n')))\n",
    "        sentences = list(map(lambda x: ' '.join(x.split('\\n')), sentences))\n",
    "        for sentence in sentences:\n",
    "            sentence_labels = model.max_marginal(sentence)\n",
    "            for idx, word in enumerate(sentence.split()):\n",
    "                out_file.write(\"{} {}\\n\".format(word, sentence_labels[idx]))\n",
    "            out_file.write('\\n')\n",
    "        out_file.close()\n",
    "        in_file.close()\n",
    "\n",
    "    print(\"Max Marginal Finished: {}\".format(l))\n",
    "    \n",
    "    output = os.popen(\n",
    "        \"python3 EvalScript/evalResult.py {0}/dev.out {0}/dev.p4.out\".format(l)).read()\n",
    "    print(\"Language: {}\".format(l))\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improvement 1: Default Emission Params\n",
    "\n",
    "The first proposed improvement to current algorithm is to set a default small emission parameters. \n",
    "We realised that a lot of time, if a word is never tagged with a specific label before in training set, the \"path\" that passes through such pair of word and label will always have a probability of 0, no matter how likely the transition between that label and the previous/next labels are.\n",
    "Also, in real life (\"the universal bag of word\"), there is always a probability, even if it's small, that a word is tagged with a specific label, it makes more sense to give all pair of word and label a default probability (emission param).\n",
    "We tested our hypothesis by implementing it as an option in our main algorithm class. We used the elbow method with a range from 10<sup>-3</sup> to 10<sup>-20</sup> to identify what is the best default param.\n",
    "\n",
    "Results on EN dataset\n",
    "![EN](./EN/score.png)\n",
    "\n",
    "Results on FR dataset\n",
    "![FR](./FR/score.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish training for EN\n",
      "----------Viterbi for EN------------\n",
      "Viterbi Finished: EN\n",
      "Language: EN\n",
      "\n",
      "#Entity in gold data: 226\n",
      "#Entity in prediction: 200\n",
      "\n",
      "#Correct Entity : 109\n",
      "Entity  precision: 0.5450\n",
      "Entity  recall: 0.4823\n",
      "Entity  F: 0.5117\n",
      "\n",
      "#Correct Sentiment : 69\n",
      "Sentiment  precision: 0.3450\n",
      "Sentiment  recall: 0.3053\n",
      "Sentiment  F: 0.3239\n",
      "\n",
      "Finish training for FR\n",
      "----------Viterbi for FR------------\n",
      "Viterbi Finished: FR\n",
      "Language: FR\n",
      "\n",
      "#Entity in gold data: 223\n",
      "#Entity in prediction: 196\n",
      "\n",
      "#Correct Entity : 119\n",
      "Entity  precision: 0.6071\n",
      "Entity  recall: 0.5336\n",
      "Entity  F: 0.5680\n",
      "\n",
      "#Correct Sentiment : 80\n",
      "Sentiment  precision: 0.4082\n",
      "Sentiment  recall: 0.3587\n",
      "Sentiment  F: 0.3819\n",
      "\n"
     ]
    }
   ],
   "source": [
    "languages = ['EN', 'FR']\n",
    "\n",
    "for l in languages:\n",
    "    model = SUTDHMM(default_emission=0.000001)\n",
    "    model.train(input_filename='./{}/train'.format(l))\n",
    "\n",
    "    print(\"Finish training for {}\".format(l))\n",
    "\n",
    "    print(\"----------Viterbi for {0}------------\".format(l))\n",
    "    with open(\"./{}/dev.in\".format(l)) as in_file, open(\"./{}/dev.pdefault.out\".format(l), 'w+') as out_file:\n",
    "        read_data = in_file.read()\n",
    "        sentences = list(filter(lambda x: len(x) > 0, read_data.split('\\n\\n')))\n",
    "        sentences = list(map(lambda x: ' '.join(x.split('\\n')), sentences))\n",
    "        for sentence in sentences:\n",
    "            sentence_labels, chance = model.viterbi(sentence)\n",
    "            for idx, word in enumerate(sentence.split()):\n",
    "                out_file.write(\"{} {}\\n\".format(word, sentence_labels[idx]))\n",
    "            out_file.write('\\n')\n",
    "        out_file.close()\n",
    "        in_file.close()\n",
    "\n",
    "    print(\"Viterbi Finished: {}\".format(l))\n",
    "    \n",
    "    output = os.popen(\n",
    "        \"python3 EvalScript/evalResult.py {0}/dev.out {0}/dev.pdefault.out\".format(l)).read()\n",
    "    print(\"Language: {}\".format(l))\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Conclusion__: The default emission params perform best for our datasets at 10<sup>-6</sup>. This method gives us a better result than our original Viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improvement 2: Predicting Entity and Sentiment separately\n",
    "\n",
    "The second proposed improvement for current algorithm is to separate the prediction of entity and sentiment. We theorize that because entity and sentiment labels are not related, so if we join them as a single label, they will affect the probability of each other and produce a lower accuracy. For example, entity B might show up more frequently in the dataset together with sentiment \"negative\", however, they are actually not related but in this case, the probability of predicting B-negative are significantly high compared to other tags. This imply a false correlation between the entity tag (B) and the sentiment tag (negative). Hence, predicting them separately would eliminate this false correlation thus produce better result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for EN\n",
      "Language: EN\n",
      "\n",
      "#Entity in gold data: 226\n",
      "#Entity in prediction: 115\n",
      "\n",
      "#Correct Entity : 74\n",
      "Entity  precision: 0.6435\n",
      "Entity  recall: 0.3274\n",
      "Entity  F: 0.4340\n",
      "\n",
      "#Correct Sentiment : 53\n",
      "Sentiment  precision: 0.4609\n",
      "Sentiment  recall: 0.2345\n",
      "Sentiment  F: 0.3109\n",
      "\n",
      "Finished training for FR\n",
      "Language: FR\n",
      "\n",
      "#Entity in gold data: 223\n",
      "#Entity in prediction: 104\n",
      "\n",
      "#Correct Entity : 74\n",
      "Entity  precision: 0.7115\n",
      "Entity  recall: 0.3318\n",
      "Entity  F: 0.4526\n",
      "\n",
      "#Correct Sentiment : 49\n",
      "Sentiment  precision: 0.4712\n",
      "Sentiment  recall: 0.2197\n",
      "Sentiment  F: 0.2997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "languages = ['EN', 'FR']\n",
    "\n",
    "for l in languages:\n",
    "    model = SUTDHMM()\n",
    "    model.load_data(data_filename='./{}/train'.format(l))\n",
    "    with open('./{}/train.ent'.format(l), 'w+') as ent_in_file:\n",
    "        for token in model.tokens_list:\n",
    "            word = token[0]\n",
    "            tag = token[1].split(\n",
    "                '-')[0] if token[1] not in ['O', 'START', 'STOP'] else token[1]\n",
    "            ent_in_file.write('{} {}\\n'.format(word, tag))\n",
    "            if token[1] == 'STOP':\n",
    "                ent_in_file.write('\\n')\n",
    "        ent_in_file.close()\n",
    "    with open('./{}/train.sen'.format(l), 'w+') as sen_in_file:\n",
    "        for token in model.tokens_list:\n",
    "            word = token[0]\n",
    "            tag = token[1].split(\n",
    "                '-')[1] if token[1] not in ['O', 'START', 'STOP'] else token[1]\n",
    "            sen_in_file.write('{} {}\\n'.format(word, tag))\n",
    "            if token[1] == 'STOP':\n",
    "                sen_in_file.write('\\n')\n",
    "        sen_in_file.close()\n",
    "\n",
    "    ent_model = SUTDHMM()\n",
    "    ent_model.train(input_filename='./{}/train.ent'.format(l))\n",
    "    sen_model = SUTDHMM()\n",
    "    sen_model.train(input_filename='./{}/train.sen'.format(l))\n",
    "    print('Finished training for {}'.format(l))\n",
    "    with open('./{}/dev.in'.format(l)) as in_file, open('./{}/dev.psep.out'.format(l), 'w+') as out_file:\n",
    "        read_data = in_file.read()\n",
    "        sentences = list(filter(lambda x: len(x) > 0, read_data.split('\\n\\n')))\n",
    "        sentences = list(map(lambda x: ' '.join(x.split('\\n')), sentences))\n",
    "        for sentence in sentences:\n",
    "            sentence_ent, prob = ent_model.viterbi(sentence=sentence)\n",
    "            sentence_sen, prob = sen_model.viterbi(sentence=sentence)\n",
    "            for idx in range(0, len(sentence_ent)):\n",
    "                entity = sentence_ent[idx]\n",
    "                sentiment = sentence_sen[idx]\n",
    "                if entity not in ['O', 'START', 'STOP'] and sentiment not in ['O', 'START', 'STOP']:\n",
    "                    out_file.write(\n",
    "                        \"{} {}-{}\\n\".format(word, entity, sentiment))\n",
    "                elif entity in ['O', 'START', 'STOP']:\n",
    "                    out_file.write(\"{} {}\\n\".format(word, entity))\n",
    "                else:\n",
    "                    out_file.write('{} {}\\n'.format(word, sentiment))\n",
    "            out_file.write('\\n')\n",
    "    \n",
    "    output = os.popen(\"python3 EvalScript/evalResult.py {0}/dev.out {0}/dev.psep.out\".format(l)).read()\n",
    "    print(\"Language: {}\".format(l))\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Conclusion__: This method only gives a slightly better result on current datasets. But it would make a whole lot of difference if the datasets are skewed towards certains pair of tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improvement 3\n",
    "\n",
    "The third proposed improvement to the current algorithm is to learn implement the discriminative training methods using perceptron (adopted from this [paper](http://www.aclweb.org/anthology/W02-1001)). The basic steps of implementation are as below:\n",
    "\n",
    "![steps](perceptron-steps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented it inside our main class ```SUTDHMM``` as a special training method ```train_perceptron``` and a special predicting method ```predict_perceptron```. Please investigate the code for further understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish training for EN\n",
      "----------Perceptron for EN------------\n",
      "Perceptron Finished: EN\n",
      "Language: EN\n",
      "\n",
      "#Entity in gold data: 226\n",
      "#Entity in prediction: 103\n",
      "\n",
      "#Correct Entity : 63\n",
      "Entity  precision: 0.6117\n",
      "Entity  recall: 0.2788\n",
      "Entity  F: 0.3830\n",
      "\n",
      "#Correct Sentiment : 45\n",
      "Sentiment  precision: 0.4369\n",
      "Sentiment  recall: 0.1991\n",
      "Sentiment  F: 0.2736\n",
      "\n",
      "Finish training for FR\n",
      "----------Perceptron for FR------------\n",
      "Perceptron Finished: FR\n",
      "Language: FR\n",
      "\n",
      "#Entity in gold data: 223\n",
      "#Entity in prediction: 109\n",
      "\n",
      "#Correct Entity : 78\n",
      "Entity  precision: 0.7156\n",
      "Entity  recall: 0.3498\n",
      "Entity  F: 0.4699\n",
      "\n",
      "#Correct Sentiment : 51\n",
      "Sentiment  precision: 0.4679\n",
      "Sentiment  recall: 0.2287\n",
      "Sentiment  F: 0.3072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "languages = ['EN', 'FR']\n",
    "\n",
    "for l in languages:\n",
    "    model = SUTDHMM()\n",
    "    model.train_perceptron(input_filename='./{}/train'.format(l))\n",
    "    print(\"Finish training for {}\".format(l))\n",
    "\n",
    "    print(\"----------Perceptron for {0}------------\".format(l))\n",
    "    with open(\"./{}/dev.in\".format(l)) as in_file, open(\"./{}/dev.perceptron.out\".format(l), 'w+') as out_file:\n",
    "        read_data = in_file.read()\n",
    "        sentences = list(\n",
    "            filter(lambda x: len(x) > 0, read_data.split('\\n\\n')))\n",
    "        sentences = list(map(lambda x: ' '.join(x.split('\\n')), sentences))\n",
    "        for sentence in sentences:\n",
    "            sentence_labels, chance = model.predict_perceptron(sentence)\n",
    "            for idx, word in enumerate(sentence.split()):\n",
    "                out_file.write(\"{} {}\\n\".format(\n",
    "                    word, sentence_labels[idx]))\n",
    "            out_file.write('\\n')\n",
    "        out_file.close()\n",
    "        in_file.close()\n",
    "\n",
    "    print(\"Perceptron Finished: {}\".format(l))\n",
    "\n",
    "    output = os.popen(\n",
    "        \"python3 EvalScript/evalResult.py {0}/dev.out {0}/dev.perceptron.out\".format(l)).read()\n",
    "    print(\"Language: {}\".format(l))\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Conclusion__: This proposed method doesnot improve the accuracy for our specific usecase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combination\n",
    "\n",
    "Finally, we would like to attempt to combine our previously proposed improvement to the algorithm design. We found that the best combination is method 1 and method 2 together. The implementation is as below, which yields the best results on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for EN\n",
      "Language: EN\n",
      "\n",
      "#Entity in gold data: 226\n",
      "#Entity in prediction: 196\n",
      "\n",
      "#Correct Entity : 106\n",
      "Entity  precision: 0.5408\n",
      "Entity  recall: 0.4690\n",
      "Entity  F: 0.5024\n",
      "\n",
      "#Correct Sentiment : 69\n",
      "Sentiment  precision: 0.3520\n",
      "Sentiment  recall: 0.3053\n",
      "Sentiment  F: 0.3270\n",
      "\n",
      "Finished training for SG\n",
      "Language: SG\n",
      "\n",
      "#Entity in gold data: 1382\n",
      "#Entity in prediction: 1003\n",
      "\n",
      "#Correct Entity : 460\n",
      "Entity  precision: 0.4586\n",
      "Entity  recall: 0.3329\n",
      "Entity  F: 0.3857\n",
      "\n",
      "#Correct Sentiment : 278\n",
      "Sentiment  precision: 0.2772\n",
      "Sentiment  recall: 0.2012\n",
      "Sentiment  F: 0.2331\n",
      "\n",
      "Finished training for CN\n",
      "Language: CN\n",
      "\n",
      "#Entity in gold data: 362\n",
      "#Entity in prediction: 219\n",
      "\n",
      "#Correct Entity : 45\n",
      "Entity  precision: 0.2055\n",
      "Entity  recall: 0.1243\n",
      "Entity  F: 0.1549\n",
      "\n",
      "#Correct Sentiment : 29\n",
      "Sentiment  precision: 0.1324\n",
      "Sentiment  recall: 0.0801\n",
      "Sentiment  F: 0.0998\n",
      "\n",
      "Finished training for FR\n",
      "Language: FR\n",
      "\n",
      "#Entity in gold data: 223\n",
      "#Entity in prediction: 167\n",
      "\n",
      "#Correct Entity : 109\n",
      "Entity  precision: 0.6527\n",
      "Entity  recall: 0.4888\n",
      "Entity  F: 0.5590\n",
      "\n",
      "#Correct Sentiment : 76\n",
      "Sentiment  precision: 0.4551\n",
      "Sentiment  recall: 0.3408\n",
      "Sentiment  F: 0.3897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "languages = ['EN', 'SG', 'CN', 'FR']\n",
    "\n",
    "for l in languages:\n",
    "    model = SUTDHMM()\n",
    "    model.load_data(data_filename='./{}/train'.format(l))\n",
    "    with open('./{}/train.ent'.format(l), 'w+') as ent_in_file:\n",
    "        for token in model.tokens_list:\n",
    "            word = token[0]\n",
    "            tag = token[1].split(\n",
    "                '-')[0] if token[1] not in ['O', 'START', 'STOP'] else token[1]\n",
    "            ent_in_file.write('{} {}\\n'.format(word, tag))\n",
    "            if token[1] == 'STOP':\n",
    "                ent_in_file.write('\\n')\n",
    "        ent_in_file.close()\n",
    "    with open('./{}/train.sen'.format(l), 'w+') as sen_in_file:\n",
    "        for token in model.tokens_list:\n",
    "            word = token[0]\n",
    "            tag = token[1].split(\n",
    "                '-')[1] if token[1] not in ['O', 'START', 'STOP'] else token[1]\n",
    "            sen_in_file.write('{} {}\\n'.format(word, tag))\n",
    "            if token[1] == 'STOP':\n",
    "                sen_in_file.write('\\n')\n",
    "        sen_in_file.close()\n",
    "\n",
    "    ent_model = SUTDHMM(default_emission=0.0000001)\n",
    "    ent_model.train(input_filename='./{}/train.ent'.format(l))\n",
    "    sen_model = SUTDHMM(default_emission=0.0000001)\n",
    "    sen_model.train(input_filename='./{}/train.sen'.format(l))\n",
    "    print('Finished training for {}'.format(l))\n",
    "    with open('./{}/dev.in'.format(l)) as in_file, open('./{}/dev.pdefaultsep.out'.format(l), 'w+') as out_file:\n",
    "        read_data = in_file.read()\n",
    "        sentences = list(filter(lambda x: len(x) > 0, read_data.split('\\n\\n')))\n",
    "        sentences = list(map(lambda x: ' '.join(x.split('\\n')), sentences))\n",
    "        for sentence in sentences:\n",
    "            sentence_ent, prob = ent_model.viterbi(sentence=sentence)\n",
    "            sentence_sen, prob = sen_model.viterbi(sentence=sentence)\n",
    "            for idx in range(0, len(sentence_ent)):\n",
    "                entity = sentence_ent[idx]\n",
    "                sentiment = sentence_sen[idx]\n",
    "                if entity not in ['O', 'START', 'STOP'] and sentiment not in ['O', 'START', 'STOP']:\n",
    "                    out_file.write(\n",
    "                        \"{} {}-{}\\n\".format(word, entity, sentiment))\n",
    "                elif entity in ['O', 'START', 'STOP']:\n",
    "                    out_file.write(\"{} {}\\n\".format(word, entity))\n",
    "                else:\n",
    "                    out_file.write('{} {}\\n'.format(word, sentiment))\n",
    "            out_file.write('\\n')\n",
    "    \n",
    "    output = os.popen(\"python3 EvalScript/evalResult.py {0}/dev.out {0}/dev.pdefaultsep.out\".format(l)).read()\n",
    "    print(\"Language: {}\".format(l))\n",
    "    print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
