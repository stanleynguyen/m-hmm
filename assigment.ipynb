{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Estimate the emission parameters from the training set using MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "class Emission:\n",
    "    def __init__(self):\n",
    "        self.emission_p = {}\n",
    "        self.y_count = {}\n",
    "        self.y_labels = []\n",
    "        self.x_given_y_count = {}\n",
    "        self.tokens_list = []\n",
    "        self.special_token = '#UNK#'\n",
    "    \n",
    "    def clean_data(self, k = 1):\n",
    "        token_freq = {}\n",
    "        for token in self.tokens_list:\n",
    "            if token[0] not in token_freq: \n",
    "                token_freq[token[0]] = 1\n",
    "            else:\n",
    "                token_freq[token[0]] += 1\n",
    "        for i in range(len(self.tokens_list)):\n",
    "            if token_freq[self.tokens_list[i][0]] < k:\n",
    "                self.tokens_list[i][0] = self.special_token\n",
    "        return self.tokens_list\n",
    "    \n",
    "    def train(self, tokens_list: list, k = 1, special_token = '#UNK#'):\n",
    "        self.tokens_list = tokens_list\n",
    "        self.special_token = special_token\n",
    "        self.clean_data(k)\n",
    "        self.y_count = {}\n",
    "        self.x_given_y_count = {} \n",
    "\n",
    "        for token in tokens_list:\n",
    "            if token[0] in ['START', 'STOP']:\n",
    "                continue\n",
    "            if token[1] not in self.y_count:\n",
    "                self.y_count[token[1]] = 1\n",
    "                self.x_given_y_count[token[1]] = {}\n",
    "                self.x_given_y_count[token[1]][token[0]] = 1\n",
    "            else:\n",
    "                self.y_count[token[1]] += 1\n",
    "                if token[0] not in self.x_given_y_count[token[1]]:\n",
    "                    self.x_given_y_count[token[1]][token[0]] = 1\n",
    "                else:\n",
    "                    self.x_given_y_count[token[1]][token[0]] += 1\n",
    "\n",
    "        # calculate emission params\n",
    "        self.emission_p = copy.deepcopy(self.x_given_y_count)\n",
    "        for label in self.emission_p:\n",
    "            for word in self.emission_p[label]:\n",
    "                self.emission_p[label][word] = float(self.x_given_y_count[label][word]) / self.y_count[label]\n",
    "            if k == 1: self.emission_p[label][self.special_token] = 0\n",
    "        self.y_labels = list(self.emission_p.keys())\n",
    "        return self.emission_p\n",
    "    \n",
    "    def predict(self, y: str, x: str):\n",
    "        x_inside = False\n",
    "        for token in self.tokens_list:\n",
    "            if token[0] == x:\n",
    "                x_inside = True\n",
    "                break\n",
    "\n",
    "        if not x_inside:\n",
    "            x = self.special_token\n",
    "\n",
    "        if x not in self.emission_p[y]:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.emission_p[y][x]\n",
    "        \n",
    "    def predict_tag(self, x: str):\n",
    "        score = 0.0\n",
    "        y_tag = None\n",
    "        for y in self.y_labels:\n",
    "            y_score = self.predict(y, x)\n",
    "            if y_score > score:\n",
    "                y_tag = y\n",
    "                score = y_score\n",
    "        return y_tag\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': {'a': 0.3333333333333333, 'b': 0.3333333333333333, 'c': 0.3333333333333333, '#UNK#': 0}, 'I': {'a': 1.0, '#UNK#': 0}}\n",
      "0.3333333333333333\n",
      "['O', 'I']\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "# Example ussage\n",
    "import os\n",
    "\n",
    "data = [['a', 'O'], ['b', 'O'], ['a', 'I'], ['c', 'O']]\n",
    "model = Emission()\n",
    "print(model.train(tokens_list=data))\n",
    "print(model.predict(y='O', x='b'))\n",
    "print(model.y_labels)\n",
    "print(model.predict_tag('a'))\n",
    "\n",
    "# with open('./EN/train') as train_file:\n",
    "#     read_data = train_file.read()\n",
    "#     read_data = os.linesep.join([s for s in read_data.splitlines() if s])\n",
    "#     data = list(map(lambda x: x.split(' '),read_data.split('\\n')))\n",
    "# emission_params(tokens_list=data, y='O', x='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify training set to replace words that appear less than k times with special token. Apply this to the emission parameters prediction function with k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "data = [['a', 'O'], ['b', 'O'], ['a', 'I'], ['c', 'O'], ['a', 'O']]\n",
    "model = Emission()\n",
    "model.train(tokens_list=data, k = 3)\n",
    "model.predict(y='O', x='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis system that produces the tag for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish training for EN\n",
      "Finished: EN\n",
      "Finish training for SG\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-59eaedd36f2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mout_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mout_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} {}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-b1014383c0a8>\u001b[0m in \u001b[0;36mpredict_tag\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0my_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my_score\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0my_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-b1014383c0a8>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, y, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mx_inside\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mx_inside\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "languages = ['EN', 'SG', 'CN', 'FR']\n",
    "\n",
    "for l in languages:\n",
    "    model = Emission()\n",
    "    with open(\"./{}/train\".format(l)) as train_file:\n",
    "        read_data = train_file.read()\n",
    "        read_data = os.linesep.join([s if s else 'START START\\nSTOP STOP' for s in read_data.splitlines()])\n",
    "        data = list(map(lambda x: x.rsplit(' ',1),read_data.split('\\n')))\n",
    "        model.train(tokens_list=data, k=3)\n",
    "    \n",
    "    print(\"Finish training for {}\".format(l))\n",
    "\n",
    "    with open(\"./{}/dev.in\".format(l)) as in_file, open(\"./{}/dev.p2.out\".format(l), 'w+') as out_file:\n",
    "        for line in in_file:\n",
    "            word = line.strip()\n",
    "            if (word == ''):\n",
    "                out_file.write(\"\\n\")\n",
    "            else:\n",
    "                out_file.write(\"{} {}\\n\".format(word, model.predict_tag(word)))\n",
    "    print(\"Finished: {}\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for l in languages:\n",
    "    output = os.popen(\"python3 EvalScript/evalResult.py {0}/dev.out {0}/dev.p2.out\".format(l)).read()\n",
    "    print(\"Language: {}\".format(l))\n",
    "    print(output)\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimates the transition parameters from the training set using MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "def transition_params(ordered_labels_list: list):\n",
    "    count = {}\n",
    "    count_given = {} # 2 layer dictionary depth-0 key is the (i-1)-label, depth-1 key is the i-label\n",
    "    \n",
    "    # count frequency of all label and combinations of 2 labels in the dataset\n",
    "    for idx, label in enumerate(ordered_labels_list):\n",
    "        if label == 'STOP':\n",
    "            continue\n",
    "        if label not in count:\n",
    "            count[label] = 1\n",
    "            count_given[label] = {}\n",
    "            if idx < len(ordered_labels_list) - 1:\n",
    "                next_label = ordered_labels_list[idx + 1]\n",
    "                count_given[label][next_label] = 1\n",
    "        else:\n",
    "            count[label] += 1\n",
    "            if idx < len(ordered_labels_list) - 1:\n",
    "                next_label = ordered_labels_list[idx + 1]\n",
    "                if next_label not in count_given[label]:\n",
    "                    count_given[label][next_label] = 1\n",
    "                else:\n",
    "                    count_given[label][next_label] += 1\n",
    "    \n",
    "    # calculate trans_params\n",
    "    trans_params = copy.deepcopy(count_given)\n",
    "    for given_label in trans_params:\n",
    "        for label in trans_params[given_label]:\n",
    "            trans_params[given_label][label] /= count[given_label]\n",
    "    \n",
    "    return trans_params\n",
    "\n",
    "def specific_transition_params(ordered_labels_list: list, y: str, y_given: str):\n",
    "    trans_params = transition_params(ordered_labels_list)\n",
    "    if y not in trans_params:\n",
    "        return 0;\n",
    "    elif y_given not in trans_params[y]:\n",
    "        return 0;\n",
    "    else:\n",
    "        return trans_params[y_given][y]\n",
    "    \n",
    "specific_transition_params(['START','a', 'b', 'b', 'STOP','START','c', 'b', 'a', 'd', 'h', 'b','STOP'], 'b', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viterbi algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Z', 'Y'], 0.005291005291005291)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def viterbi(sentence: str, labels: list, trans_p: dict, emission_p: dict):\n",
    "    observed_words = sentence.split()\n",
    "    cache = [{}]\n",
    "    \n",
    "    # handle first layer\n",
    "    for l in labels:\n",
    "        trans_param = trans_p['START'][l] if l in trans_p['START'] else 0\n",
    "        emission_param = emission_p[l][observed_words[0]] if observed_words[0] in emission_p[l] else emission_p[l]['#UNK#']\n",
    "        cache[0][l] = {\"chance\": trans_param * emission_param, \"prev\": None}\n",
    "    \n",
    "    # handle middle layers\n",
    "    for i in range(1, len(observed_words)):\n",
    "        cache.append({})\n",
    "        for l in labels:\n",
    "            max_trans_prob = -math.inf\n",
    "            max_prev_l = None\n",
    "            for prev_l in labels:\n",
    "                trans_param = trans_p[prev_l][l] if l in trans_p[prev_l] else 0\n",
    "                trans_prob = cache[i-1][prev_l]['chance'] * trans_param\n",
    "                if trans_prob > max_trans_prob:\n",
    "                    max_trans_prob = trans_prob\n",
    "                    max_prev_l = prev_l\n",
    "            \n",
    "            emission_param = emission_p[l][observed_words[i]] if observed_words[i] in emission_p[l] else emission_p[l]['#UNK#']\n",
    "            cache[i][l] = {'chance': max_trans_prob * emission_param, 'prev': max_prev_l}\n",
    "            \n",
    "    # handle the end layer       \n",
    "    cache.append({})\n",
    "    max_end_prob = -math.inf\n",
    "    max_end_l = None\n",
    "    for l in labels:\n",
    "        trans_param = trans_p[l]['STOP'] if 'STOP' in trans_p[l] else 0\n",
    "        end_prob = cache[len(observed_words) - 1][l]['chance'] * trans_param\n",
    "        if end_prob > max_end_prob:\n",
    "            max_end_prob = end_prob\n",
    "            max_end_l = l\n",
    "    cache[len(observed_words)]['STOP'] = {'chance': max_end_prob, 'prev': max_end_l}\n",
    "    \n",
    "    # backtrack for optimal path\n",
    "    optimal_prob = cache[len(observed_words)]['STOP']['chance']\n",
    "    previous_l = cache[len(observed_words)]['STOP']['prev']\n",
    "    optimal = [previous_l]\n",
    "    for i in range(len(observed_words) -1, 0, -1):\n",
    "        optimal.insert(0, cache[i][previous_l]['prev'])\n",
    "        previous_l = cache[i][previous_l]['prev']\n",
    "        \n",
    "    return (optimal, optimal_prob)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "print(viterbi(\n",
    "    'a d',\n",
    "    ['X', 'Y', 'Z'],\n",
    "    {\n",
    "        'START': {'X': 3/7, 'Z': 4/7},\n",
    "        'X': {'X': 2/7, 'Z': 3/7, 'STOP': 2/7},\n",
    "        'Y': {'X': 0.25, 'STOP': 0.75},\n",
    "        'Z': {'X': 1/9, 'Y': 4/9, 'Z': 2/9, 'STOP': 2/9}\n",
    "    },\n",
    "    {\n",
    "        'X': {'a': 3/7, 'b': 2/7, 'c': 2/7, '#UNK#': 0},\n",
    "        'Y': {'a': 0.5, 'c': 0.25, 'd': 0.25, '#UNK#': 0},\n",
    "        'Z': {'a': 1/9, 'b': 5/9, 'c': 1/9, 'd': 2/9, '#UNK#': 0}\n",
    "    }\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['O', 'O', 'O', 'O', 'O'], 0.0)\n",
      "Finish training for EN\n",
      "Finished: EN\n",
      "(['O', 'O', 'O', 'O', 'O'], 0.0)\n",
      "Finish training for SG\n",
      "Finished: SG\n",
      "(['O', 'O', 'O', 'O', 'O'], 0.0)\n",
      "Finish training for CN\n",
      "Finished: CN\n",
      "(['O', 'O', 'O', 'O', 'O'], 0.0)\n",
      "Finish training for FR\n",
      "Finished: FR\n"
     ]
    }
   ],
   "source": [
    "languages = ['EN', 'SG', 'CN', 'FR']\n",
    "\n",
    "for l in languages:\n",
    "    model = Emission()\n",
    "    with open(\"./{}/train\".format(l)) as train_file:\n",
    "        read_data = train_file.read()\n",
    "        read_data = os.linesep.join([s if s else 'STOP STOP\\nSTART START' for s in read_data.splitlines()])\n",
    "        data = list(map(lambda x: x.rsplit(' ',1),read_data.split('\\n')))\n",
    "        model.train(tokens_list=data, k=1)\n",
    "        emission_p = model.emission_p\n",
    "        labels = model.y_labels\n",
    "        ordered_labels_list = list(map(lambda x: x[1], data))\n",
    "        transition_p = transition_params(ordered_labels_list)\n",
    "\n",
    "        print(viterbi(sentence='hello my name is stanley', labels=labels, trans_p=transition_p, emission_p=emission_p))\n",
    "    \n",
    "    print(\"Finish training for {}\".format(l))\n",
    "\n",
    "    with open(\"./{}/dev.in\".format(l)) as in_file, open(\"./{}/dev.p3.out\".format(l), 'w+') as out_file:\n",
    "        read_data = in_file.read()\n",
    "        sentences = list(filter(lambda x: len(x) > 0, read_data.split('\\n\\n')))\n",
    "        sentences = list(map(lambda x: ' '.join(x.split('\\n')), sentences))\n",
    "        for sentence in sentences:\n",
    "            sentence_labels, chance = viterbi(sentence=sentence, labels=labels, trans_p=transition_p, emission_p=emission_p)\n",
    "            for idx,word in enumerate(sentence.split()):\n",
    "                out_file.write(\"{} {}\\n\".format(word, sentence_labels[idx]))\n",
    "            out_file.write('\\n')\n",
    "    print(\"Finished: {}\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: EN\n",
      "\n",
      "#Entity in gold data: 226\n",
      "#Entity in prediction: 341\n",
      "\n",
      "#Correct Entity : 2\n",
      "Entity  precision: 0.0059\n",
      "Entity  recall: 0.0088\n",
      "Entity  F: 0.0071\n",
      "\n",
      "#Correct Sentiment : 2\n",
      "Sentiment  precision: 0.0059\n",
      "Sentiment  recall: 0.0088\n",
      "Sentiment  F: 0.0071\n",
      "\n",
      "----------------------\n",
      "Language: SG\n",
      "\n",
      "#Entity in gold data: 1382\n",
      "#Entity in prediction: 2612\n",
      "\n",
      "#Correct Entity : 34\n",
      "Entity  precision: 0.0130\n",
      "Entity  recall: 0.0246\n",
      "Entity  F: 0.0170\n",
      "\n",
      "#Correct Sentiment : 26\n",
      "Sentiment  precision: 0.0100\n",
      "Sentiment  recall: 0.0188\n",
      "Sentiment  F: 0.0130\n",
      "\n",
      "----------------------\n",
      "Language: CN\n",
      "\n",
      "#Entity in gold data: 362\n",
      "#Entity in prediction: 230\n",
      "\n",
      "#Correct Entity : 0\n",
      "Entity  precision: 0.0000\n",
      "Entity  recall: 0.0000\n",
      "Entity  F: 0.0000\n",
      "\n",
      "#Correct Sentiment : 0\n",
      "Sentiment  precision: 0.0000\n",
      "Sentiment  recall: 0.0000\n",
      "Sentiment  F: 0.0000\n",
      "\n",
      "----------------------\n",
      "Language: FR\n",
      "\n",
      "#Entity in gold data: 223\n",
      "#Entity in prediction: 233\n",
      "\n",
      "#Correct Entity : 0\n",
      "Entity  precision: 0.0000\n",
      "Entity  recall: 0.0000\n",
      "Entity  F: 0.0000\n",
      "\n",
      "#Correct Sentiment : 0\n",
      "Sentiment  precision: 0.0000\n",
      "Sentiment  recall: 0.0000\n",
      "Sentiment  F: 0.0000\n",
      "\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "for l in languages:\n",
    "    output = os.popen(\"python3 EvalScript/evalResult.py {0}/dev.out {0}/dev.p3.out\".format(l)).read()\n",
    "    print(\"Language: {}\".format(l))\n",
    "    print(output)\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward-backward algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'X': 0.18367346938775508, 'Y': 0.0, 'Z': 0.06349206349206349},\n",
       "  {'X': 0.0, 'Y': 0.007054673721340388, 'Z': 0.020628121913080336}],\n",
       " [{'X': 0.021164021164021163, 'Y': 0.0, 'Z': 0.09430727023319616},\n",
       "  {'X': 0.2857142857142857, 'Y': 0.75, 'Z': 0.2222222222222222}])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "def forward_backward(sentence: str, labels: list, trans_p: dict, emission_p: dict):\n",
    "    observed_words = sentence.split()\n",
    "    \n",
    "    # forward part\n",
    "    forward = []\n",
    "    prev_forward = {}\n",
    "    for i, word in enumerate(observed_words):\n",
    "        curr_forward = {}\n",
    "        for l in labels:\n",
    "            prev_f_sum = 0\n",
    "            if i == 0:\n",
    "                trans_prob = trans_p['START'][l] if l in trans_p['START'] else 0\n",
    "                prev_f_sum = trans_prob\n",
    "            else:\n",
    "                for prev_l in labels:\n",
    "                    trans_prob = trans_p[prev_l][l] if l in trans_p[prev_l] else 0\n",
    "                    prev_f_sum += prev_forward[prev_l] * trans_prob\n",
    "            \n",
    "            emission_prob = emission_p[l][word] if word in emission_p[l] else emission_p[l]['#UNK#']\n",
    "            curr_forward[l] = emission_prob * prev_f_sum\n",
    "        \n",
    "        forward.append(curr_forward)\n",
    "        prev_forward = copy.deepcopy(curr_forward)\n",
    "    \n",
    "    forward_prob = 0\n",
    "    for l in labels:\n",
    "        trans_prob = trans_p[l]['STOP'] if 'STOP' in trans_p[l] else 0\n",
    "        forward_prob += curr_forward[l] * trans_prob\n",
    "    \n",
    "    # backward part\n",
    "    backward = []\n",
    "    prev_backward = {}\n",
    "    for i, word in enumerate((observed_words[1:] + [None])[::-1]):\n",
    "        curr_backward = {}\n",
    "        for l in labels:\n",
    "            curr_backward[l] = 0\n",
    "            if i == 0:\n",
    "                trans_prob = trans_p[l]['STOP'] if 'STOP' in trans_p[l] else 0\n",
    "                curr_backward[l] = trans_prob\n",
    "            else:\n",
    "                for next_l in labels:\n",
    "                    trans_prob = trans_p[l][next_l] if next_l in trans_p[l] else 0\n",
    "                    emm_prob = emission_p[next_l][word] if word in emission_p[next_l] else emission_p[next_l]['#UNK#']\n",
    "                    curr_backward[l] += trans_prob * emm_prob * prev_backward[next_l]\n",
    "        \n",
    "        backward.insert(0, curr_backward)\n",
    "        prev_backward = copy.deepcopy(curr_backward)\n",
    "    \n",
    "    backward_prob = 0\n",
    "    for l in labels:\n",
    "        trans_prob = trans_p['START'][l] if l in trans_p['START'] else 0\n",
    "        emm_prob = emission_p[l][observed_words[0]] if observed_words[0] in emission_p[l] else emission_p[l]['#UNK#']\n",
    "        backward_prob += trans_prob * emm_prob * curr_backward[l]\n",
    "        \n",
    "    return forward, backward\n",
    "\n",
    "forward_backward(\n",
    "    'a d',\n",
    "    ['X', 'Y', 'Z'],\n",
    "    {\n",
    "        'START': {'X': 3/7, 'Z': 4/7},\n",
    "        'X': {'X': 2/7, 'Z': 3/7, 'STOP': 2/7},\n",
    "        'Y': {'X': 0.25, 'STOP': 0.75},\n",
    "        'Z': {'X': 1/9, 'Y': 4/9, 'Z': 2/9, 'STOP': 2/9}\n",
    "    },\n",
    "    {\n",
    "        'X': {'a': 3/7, 'b': 2/7, 'c': 2/7, '#UNK#': 0},\n",
    "        'Y': {'a': 0.5, 'c': 0.25, 'd': 0.25, '#UNK#': 0},\n",
    "        'Z': {'a': 1/9, 'b': 5/9, 'c': 1/9, 'd': 2/9, '#UNK#': 0}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max-Marginal Decoding with Forward-backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Z', 'Y']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max_marginal(sentence: str, labels: list, trans_p: dict, emission_p: dict):\n",
    "    forward_p, backward_p = forward_backward(sentence, labels, trans_p, emission_p)\n",
    "    predictions = []\n",
    "    for i in range(len(forward_p)):\n",
    "        product_p = {l: forward_p[i][l] * backward_p[i][l] for l in labels}\n",
    "        predictions.append(max(product_p, key=product_p.get))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "max_marginal(\n",
    "    'a d',\n",
    "    ['X', 'Y', 'Z'],\n",
    "    {\n",
    "        'START': {'X': 3/7, 'Z': 4/7},\n",
    "        'X': {'X': 2/7, 'Z': 3/7, 'STOP': 2/7},\n",
    "        'Y': {'X': 0.25, 'STOP': 0.75},\n",
    "        'Z': {'X': 1/9, 'Y': 4/9, 'Z': 2/9, 'STOP': 2/9}\n",
    "    },\n",
    "    {\n",
    "        'X': {'a': 3/7, 'b': 2/7, 'c': 2/7, '#UNK#': 0},\n",
    "        'Y': {'a': 0.5, 'c': 0.25, 'd': 0.25, '#UNK#': 0},\n",
    "        'Z': {'a': 1/9, 'b': 5/9, 'c': 1/9, 'd': 2/9, '#UNK#': 0}\n",
    "    }\n",
    ")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish training for EN\n",
      "Finished: EN\n",
      "Finish training for SG\n",
      "Finished: SG\n",
      "Finish training for CN\n",
      "Finished: CN\n",
      "Finish training for FR\n",
      "Finished: FR\n"
     ]
    }
   ],
   "source": [
    "for l in languages:\n",
    "    model = Emission()\n",
    "    with open(\"./{}/train\".format(l)) as train_file:\n",
    "        read_data = train_file.read()\n",
    "        read_data = os.linesep.join([s if s else 'STOP STOP\\nSTART START' for s in read_data.splitlines()])\n",
    "        data = list(map(lambda x: x.rsplit(' ',1),read_data.split('\\n')))\n",
    "        model.train(tokens_list=data, k=3)\n",
    "        emission_p = model.emission_p\n",
    "        ordered_labels_list = list(map(lambda x: x[1], data))\n",
    "        transition_p = transition_params(ordered_labels_list)\n",
    "        labels = list(filter(lambda a: a != 'START', transition_p.keys()))\n",
    "    \n",
    "    print(\"Finish training for {}\".format(l))\n",
    "\n",
    "    with open(\"./{}/dev.in\".format(l)) as in_file, open(\"./{}/dev.p4.out\".format(l), 'w+') as out_file:\n",
    "        read_data = in_file.read()\n",
    "        sentences = list(filter(lambda x: len(x) > 0, read_data.split('\\n\\n')))\n",
    "        sentences = list(map(lambda x: ' '.join(x.split('\\n')), sentences))\n",
    "        for sentence in sentences:\n",
    "            sentence_labels = max_marginal(sentence=sentence, labels=labels, trans_p=transition_p, emission_p=emission_p)\n",
    "            for idx,word in enumerate(sentence.split()):\n",
    "                out_file.write(\"{} {}\\n\".format(word, sentence_labels[idx]))\n",
    "    print(\"Finished: {}\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: EN\n",
      "\n",
      "#Entity in gold data: 226\n",
      "#Entity in prediction: 394\n",
      "\n",
      "#Correct Entity : 0\n",
      "Entity  precision: 0.0000\n",
      "Entity  recall: 0.0000\n",
      "Entity  F: 0.0000\n",
      "\n",
      "#Correct Sentiment : 0\n",
      "Sentiment  precision: 0.0000\n",
      "Sentiment  recall: 0.0000\n",
      "Sentiment  F: 0.0000\n",
      "\n",
      "----------------------\n",
      "Language: SG\n",
      "\n",
      "#Entity in gold data: 1382\n",
      "#Entity in prediction: 3208\n",
      "\n",
      "#Correct Entity : 0\n",
      "Entity  precision: 0.0000\n",
      "Entity  recall: 0.0000\n",
      "Entity  F: 0.0000\n",
      "\n",
      "#Correct Sentiment : 0\n",
      "Sentiment  precision: 0.0000\n",
      "Sentiment  recall: 0.0000\n",
      "Sentiment  F: 0.0000\n",
      "\n",
      "----------------------\n",
      "Language: CN\n",
      "\n",
      "#Entity in gold data: 362\n",
      "#Entity in prediction: 270\n",
      "\n",
      "#Correct Entity : 0\n",
      "Entity  precision: 0.0000\n",
      "Entity  recall: 0.0000\n",
      "Entity  F: 0.0000\n",
      "\n",
      "#Correct Sentiment : 0\n",
      "Sentiment  precision: 0.0000\n",
      "Sentiment  recall: 0.0000\n",
      "Sentiment  F: 0.0000\n",
      "\n",
      "----------------------\n",
      "Language: FR\n",
      "\n",
      "#Entity in gold data: 223\n",
      "#Entity in prediction: 241\n",
      "\n",
      "#Correct Entity : 0\n",
      "Entity  precision: 0.0000\n",
      "Entity  recall: 0.0000\n",
      "Entity  F: 0.0000\n",
      "\n",
      "#Correct Sentiment : 0\n",
      "Sentiment  precision: 0.0000\n",
      "Sentiment  recall: 0.0000\n",
      "Sentiment  F: 0.0000\n",
      "\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "for l in languages:\n",
    "    output = os.popen(\"python3 EvalScript/evalResult.py {0}/dev.out {0}/dev.p4.out\".format(l)).read()\n",
    "    print(\"Language: {}\".format(l))\n",
    "    print(output)\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
